# FFT Optimization Plan: Closing the Gap with FFTW

## Executive Summary

FFTW-js is ~2x faster than our implementation due to:

1. **Pre-optimized codelets** for sizes 2-64
2. **Hierarchical decomposition** using optimal codelet sizes
3. **FMA (fused multiply-add)** reducing instruction count
4. **Cache-aware memory access** patterns

This document outlines a phased approach to implement these optimizations with proper tooling and testing infrastructure.

---

## Why FFTW-js is Fast: Deep Analysis

FFTW-js is FFTW compiled to WebAssembly via Emscripten, meaning it inherits ALL of FFTW's sophisticated optimizations. Understanding these is key to closing the performance gap.

### 1. The genfft Codelet Generator

FFTW's codelets aren't just unrolled loops - they're generated by a compiler (`genfft`) that:

1. **DAG Representation**: Builds a directed acyclic graph of all operations
2. **Algebraic Simplification**: Applies constant folding, strength reduction, and algebraic identities
3. **Common Subexpression Elimination (CSE)**: Identifies and reuses repeated computations
4. **Network Transposition**: Transposes the computation graph, simplifies, transposes back - this exposes additional common subexpressions invisible to standard CSE
5. **Optimal Scheduling**: Produces a topological sort optimized for register allocation using cache-oblivious theory

**Key insight**: Their codelets have ~10-20% fewer operations than naively unrolled code due to these algebraic optimizations.

### 2. Operation Fusion (Twiddle-Butterfly Fusion)

**What we do**: Separate twiddle multiplication pass + butterfly pass

```
for each k: temp[k] = x[k] * twiddle[k]    // Load twiddle, load x, store temp
for each k: butterfly(temp[k], ...)        // Load temp again
```

**What FFTW does**: Fused twiddle-butterfly kernels

```
for each k: butterfly(x[k] * twiddle[k], ...)  // Load once, compute, store once
```

**Impact**: Eliminates N/2 memory round-trips per stage. For large FFTs, this is huge since memory bandwidth is often the bottleneck.

### 3. Automatic Real-FFT Specialization

FFTW's genfft **automatically derives** optimized real-FFT codelets from complex algorithms by exploiting conjugate symmetry properties. It doesn't just wrap a complex FFT - it generates specialized code that:

- Avoids redundant computation of conjugate pairs
- Uses real-only arithmetic where possible
- Fuses the pack/unpack steps with the FFT computation

### 4. Cache-Oblivious Recursion vs Our Iterative Approach

**Our Stockham**: Iterative, breadth-first - processes all butterflies at each stage before moving to the next

- Good for small FFTs that fit in cache
- Poor locality for large FFTs - data is touched log₂(N) times

**FFTW's approach**: Can use depth-first recursion

- Process subtrees completely before moving on
- Better cache utilization for large FFTs
- Working set stays in cache longer

### 5. The Planner (Less Relevant for WASM)

FFTW's runtime planner measures and selects algorithms. Since fftw-js is pre-compiled, it likely uses fixed plans - but those plans were optimized for the compilation target.

---

## Highest-Impact Optimizations (New Priorities)

Based on the FFTW analysis, these are the optimizations most likely to close the 2x performance gap:

### Priority A: Twiddle-Butterfly Fusion ~~(Expected: +25-40%)~~ **ALREADY IMPLEMENTED**

**Status**: ✅ Already implemented in our code. See Experiment 7.

**Original problem description** (did not apply to our code):

> Our Stockham FFT does separate passes: (1) Apply twiddles (2) Execute butterflies

**Actual implementation**: Our code already fuses twiddle multiply with butterfly in a single loop iteration:

```wat
(local.set $x1 (call $simd_cmul (local.get $x1) (local.get $w)))  ;; twiddle
(v128.store (local.get $o0) (f64x2.add (local.get $x0) (local.get $x1)))  ;; butterfly
(v128.store (local.get $o1) (f64x2.sub (local.get $x0) (local.get $x1)))
```

**Experiment result**: Tried inlining `$simd_cmul` to eliminate function call overhead - no improvement because V8 already inlines small functions.

**Conclusion**: This optimization path is exhausted. No further gains available here.

### Priority B: Fused Real-FFT Codelets ~~(Expected: +20-30% for rfft)~~ **IMPLEMENTED - UP TO +123%**

**Status**: ✅ Implemented for N=8 and N=32. See Experiment 8.

**Current problem**: Our real FFT is a wrapper:

1. Pack real data as complex
2. Run N/2 complex FFT
3. Post-process to extract real spectrum

**Solution**: Generate specialized real-FFT codelets that fuse pack + FFT + unpack.

**Results** (vs fftw-js):
| Size | Before (general) | After (fused codelet) | Improvement vs fftw-js |
| ---- | ---------------- | --------------------- | ---------------------- |
| N=8 | ~20M ops/s | 24.2M ops/s | **+123.8%** |
| N=32 | ~12M ops/s | 13.1M ops/s | **+45.7%** |

**Implementation**:

- `$rfft_8`: Fully fused codelet with inline FFT-4 and hardcoded post-processing twiddles
- `$rfft_32`: Calls `$fft_16` codelet then does hardcoded post-processing (eliminates twiddle memory loads)

### Priority C: DAG-Based Codelet Optimization (Expected: +10-20% for codelets)

**Current approach**: Manually write or naively unroll FFT code

**Solution**: Build a simple codelet generator that:

1. Constructs operation DAG from FFT algorithm
2. Applies CSE (common subexpression elimination)
3. Schedules for register pressure
4. Emits optimized WAT

**Key CSE opportunities in FFT**:

- Twiddle factors: `W_N^k` and `W_N^{N-k}` are conjugates
- Butterfly symmetry: `a+b` and `a-b` share the same inputs
- Real-data symmetry: Exploits conjugate pairs

**Implementation**:

```javascript
// tools/codelet_generator.js
class FFTCodeletGenerator {
  buildDAG(size, algorithm) {
    /* DIT or DIF Cooley-Tukey */
  }
  applyCSE(dag) {
    /* Hash-based common subexpression elimination */
  }
  schedule(dag) {
    /* Topological sort minimizing live variables */
  }
  emitWAT(scheduled) {
    /* Generate WAT with local variables */
  }
}
```

### Priority D: Depth-First Recursive FFT (Expected: +15-25% for N ≥ 1024)

**Current approach**: Iterative Stockham (breadth-first)

- Processes all N/2 butterflies at stage 1, then all at stage 2, etc.
- For N=4096: Touches all 4096 elements 12 times (log₂N stages)

**Solution**: Recursive decomposition with codelet base cases

```
fft(x, N):
  if N <= 64:
    codelet_N(x)  // Fully unrolled, fits in registers
  else:
    fft(x_even, N/2)     // Complete left subtree
    fft(x_odd, N/2)      // Complete right subtree
    combine(x, N)        // Single pass butterfly
```

**Benefits**:

- Working set of N/2 stays hot in cache during recursion
- Better temporal locality
- Naturally composable with codelets

**Challenges**:

- WebAssembly call overhead (mitigate with larger codelet base cases)
- Stack usage (mitigate with explicit stack or tail calls)

**Implementation**:

1. Create `modules/fft_recursive.wat`
2. Use codelets for N ≤ 64
3. Benchmark crossover point where recursive beats iterative

### Priority E: Register-Aware Scheduling (Expected: +5-10%)

**Problem**: WASM has unlimited locals, but V8/SpiderMonkey map to limited registers. Poor scheduling causes register spills.

**Solution**: Schedule operations to minimize live variables at any point.

**Heuristic** (from FFTW paper):

1. Build dependency DAG
2. Use "Sethi-Ullman" style numbering
3. Execute operations in order that minimizes max simultaneous live values

**Target**: Keep live values ≤ 16 (typical register file size)

### Priority F: Hierarchical Small-Codelet Composition (Expected: +10-20%) **IMPLEMENTED - LIMIT REACHED**

**Status**: ✅ Implemented for N=32, N=64, N=128, N=256, N=512, and N=1024. **N=1024 is the optimal ceiling** - extending to N=2048 made performance worse.

**Problem**: Large monolithic codelets (N≥32) have too many locals causing register spills (see Experiment 6).

**Solution**: Use small codelets (N=4, N=16) as building blocks and compose them using DIF decomposition.

**Implemented**:

- `$fft_32`: DIF decomposition using two `$fft_16_at` calls
- `$fft_64`: DIF decomposition using two `$fft_32_at` calls
- `$fft_128`: DIF decomposition using two `$fft_64_at` calls
- `$fft_256`: DIF decomposition using two `$fft_128_at` calls
- `$fft_512`: DIF decomposition using two `$fft_256_at` calls
- `$fft_1024`: DIF decomposition using two `$fft_512_at` calls (optimal limit)

```
fft_1024:
  // First pass: butterflies with W_1024^k twiddles
  for k in 0..511:
    first_half[k] = x[k] + x[k+512]
    second_half[k] = (x[k] - x[k+512]) * W_1024^k

  fft_512_at(0)     // First half
  fft_512_at(8192)  // Second half (512 * 16 bytes)
```

**Results**:

- N=64: improved from -30% to **+3.4%** vs fftw-js
- N=128: improved from -33% to **-16.8%** vs fftw-js
- N=256: improved from -17% to **+12.3%** vs fftw-js
- N=512: improved from -21% to **-15.8%** vs fftw-js
- N=1024: improved from -40% to **-26.9%** vs fftw-js
- N=2048: improved from -33% to **-31.1%** vs fftw-js (minimal gain - fft(1024) already used $fft_1024)

**Benefits**:

- Each codelet stays within register limits
- Reuses proven, optimized small codelets
- Hardcoded twiddles eliminate memory loads

**Limitations discovered**:

- ❌ Extending to `$fft_2048` made N=4096 **7% slower** due to instruction cache thrashing
- The optimal cutoff is **N=1024** - beyond this, simple loops beat hierarchical composition
- Code size grows exponentially: $fft_2048 alone would add 13,800 lines of WAT

### Priority G: Real-Only Arithmetic in Early Stages ~~(Expected: +5-15% for rfft)~~ **NOT APPLICABLE**

**Status**: ❌ Not applicable to our pack-based rfft algorithm.

**Original problem description**:

> rfft input is purely real, but we treat it as complex from the start.

**Why this doesn't apply**: Our rfft uses a "half-length" approach that packs pairs of reals as complex: `z[k] = x[2k] + i*x[2k+1]`. The "imaginary parts" are actual data values, NOT zeros. This is fundamentally different from a "full-length" approach where N real inputs are treated as N complex inputs with im=0.

**Conclusion**: The real-only optimization described in this priority requires a different rfft algorithm. Our pack-based approach already achieves the same computational savings (N/2 FFT instead of N FFT) through a different mechanism.

### Priority H: SIMD Pack/Unpack Fusion ~~(Expected: +5-10% for rfft)~~ **IMPLEMENTED - +2-8pp improvement**

**Status**: ✅ Implemented via SIMD post-processing. See Experiment 11.

**Original problem**: The post-processing loop used scalar f64 operations.

**Solution implemented**: Created `$rfft_postprocess_simd` that uses v128 SIMD operations for:

- Loading Z[k] and Z[n2-k] as v128
- Computing conjugates via v128.xor with sign mask
- Computing sum/diff with f64x2 operations
- Complex multiply using inline SIMD pattern
- Storing results as v128

**Results**:
| Size | Improvement vs scalar post-processing |
| ------ | ------------------------------------- |
| N=128 | +8.5pp (from -16.8% to -8.3%) |
| N=256 | +6.7pp (from +12.3% to +19.0%) |
| N=512 | +2.3pp (from -15.8% to -13.5%) |
| N=1024 | +5.0pp (from -26.9% to -21.9%) |
| N=2048 | +5.6pp (from -31.1% to -25.5%) |
| N=4096 | +4.1pp (from -44.2% to -40.1%) |

**Note**: The pack step itself is implicit in our algorithm (we just interpret N reals as N/2 complex). The optimization opportunity was in the post-processing, not the packing.

---

## Optimization Experiment Log

### Experiment 1: Dual-Complex f32 SIMD (2026-01-21)

**Hypothesis**: Process 2 f32 complex numbers per v128 register instead of 1, doubling SIMD throughput.

**Implementation**:

- Added `$simd_cmul_f32_dual` function for dual-complex multiply
- Modified butterfly loop to process pairs when r >= 2
- Added twiddle replication: `[w.re, w.im]` → `[w.re, w.im, w.re, w.im]`

**Result**: **FAILURE - 15-20% SLOWER**

| Size   | Before     | After      | Change |
| ------ | ---------- | ---------- | ------ |
| N=64   | 4.6M ops/s | 3.9M ops/s | -15%   |
| N=256  | 1.1M ops/s | 0.9M ops/s | -20%   |
| N=1024 | 249K ops/s | 203K ops/s | -18%   |
| N=4096 | 55K ops/s  | 46K ops/s  | -15%   |

**Analysis**: The overhead outweighed the benefits:

1. **Branch overhead**: `if (r >= 2)` check added to every group iteration
2. **Twiddle replication**: Extra shuffle instruction per group
3. **JIT interference**: More complex control flow may have prevented V8 optimizations
4. **Most work in r=1 stages**: For later stages (where l is large), r=1 and dual-complex doesn't help

**Lesson**: Simple, predictable loops optimize better than clever branching. The JIT compiler is already good at vectorization when the loop is simple.

### Experiment 2: N=8 Codelet (2026-01-21)

**Hypothesis**: Fully unrolled N=8 kernel with inline twiddles eliminates loop overhead and twiddle lookups.

**Implementation**: Attempted 3-stage unrolled kernel with hardcoded W_8^k twiddles.

**Result**: **FAILURE - Incorrect output**

**Analysis**: The Stockham FFT has complex permutation semantics:

1. Each stage reorders data differently than standard Cooley-Tukey DIT
2. The ping-pong buffer swap changes which indices map to which variables
3. Output positions after each stage are non-intuitive

**Lesson**: Codelet generation should be automated, not hand-written. A codelet generator that traces the actual algorithm would avoid these errors. This is exactly why FFTW uses `genfft`.

### Experiment 3: Radix-4 Stockham with SIMD (2026-01-21)

**Hypothesis**: Radix-4 algorithm has 50% fewer stages than radix-2 (log₄(N) vs log₂(N)), reducing memory passes. Combined with SIMD, should significantly improve throughput.

**Implementation**:

- Created `modules/fft_radix4.wat` with radix-4 Stockham algorithm
- SIMD N=4 kernel using v128 for butterfly operations
- Inlined SIMD complex multiply in the main butterfly loop
- Twiddles loaded as v128 [re, im] pairs
- Butterfly outputs computed with SIMD add/sub and shuffle for ±j multiplication

**Result**: **SUCCESS - Up to +51.3% faster than radix-2**

| Size   | Radix-2 (ops/s) | Radix-4 SIMD (ops/s) | Speedup                |
| ------ | --------------- | -------------------- | ---------------------- |
| N=4    | 24.3M           | 21.4M                | -11.9% (SIMD overhead) |
| N=16   | 10.4M           | 12.2M                | **+16.9%**             |
| N=64   | 3.5M            | 3.9M                 | **+10.5%**             |
| N=256  | 797K            | 977K                 | **+22.6%**             |
| N=1024 | 169K            | 196K                 | **+16.3%**             |
| N=4096 | 29.5K           | 44.6K                | **+51.3%**             |

**Analysis**:

1. **Stage reduction pays off**: log₄(4096) = 6 stages vs log₂(4096) = 12 stages
2. **SIMD inlining critical**: Initial version with function call overhead for `$simd_cmul` was 25-40% _slower_ than radix-2
3. **Large sizes benefit most**: Cache pressure reduction from fewer stages has compounding effect
4. **N=4 slower**: Fixed SIMD overhead dominates at tiny sizes; could special-case with scalar

**Key code pattern** (inlined SIMD complex multiply):

```wat
;; b1 = b * w1 (inlined, no function call)
(local.set $b1
  (f64x2.add
    (f64x2.mul (local.get $b)
      (i8x16.shuffle 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 (local.get $w1) (local.get $w1)))
    (f64x2.mul
      (f64x2.mul
        (i8x16.shuffle 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 (local.get $b) (local.get $b))
        (i8x16.shuffle 8 9 10 11 12 13 14 15 8 9 10 11 12 13 14 15 (local.get $w1) (local.get $w1)))
      (v128.const f64x2 -1.0 1.0))))
```

**Files created**:

- `modules/fft_radix4.wat` - Radix-4 Stockham with SIMD
- `tests/radix4.test.js` - Correctness tests
- `benchmarks/radix4.bench.js` - Performance benchmarks

### Current Performance (Final Benchmark Results)

Comparison against fft.js (best pure JS library, Radix-4 by Fedor Indutny):

| Size   | wat-fft Radix-2 | wat-fft Radix-4 | fft.js      | Radix-4 vs fft.js |
| ------ | --------------- | --------------- | ----------- | ----------------- |
| N=16   | 11.1M ops/s     | **12.2M ops/s** | 11.4M ops/s | **+7%**           |
| N=64   | 3.4M ops/s      | **3.8M ops/s**  | 2.8M ops/s  | **+37%**          |
| N=256  | 791K ops/s      | **971K ops/s**  | 561K ops/s  | **+73%**          |
| N=1024 | 166K ops/s      | **195K ops/s**  | 113K ops/s  | **+72%**          |
| N=4096 | 29.6K ops/s     | **44K ops/s**   | 23.5K ops/s | **+87%**          |

**Result**: Our Radix-4 SIMD implementation is now the fastest FFT across all tested sizes, outperforming even the best pure JS implementations by 7-87% depending on size.

Note: The original comparison target was fftw-js (FFTW via Emscripten). The numbers above use fft.js as the JS baseline. FFTW-js would likely still be faster at larger sizes due to its sophisticated codelet system and cache-oblivious algorithms.

### Experiment 4: N=16 Fully Unrolled Codelet (2026-01-21)

**Hypothesis**: Fully unrolled N=16 codelet with inline twiddle constants eliminates all loop overhead and twiddle lookups.

**Implementation**:

- Added `$fft_16` function with all 16 loads, 2 radix-4 stages fully unrolled
- Inline twiddle constants: W_16^1 through W_16^9
- No loops, no twiddle table lookups

**Result**: **SUCCESS - +54.6% speedup at N=16**

| Size | Before N=16 Codelet           | After N=16 Codelet | Improvement           |
| ---- | ----------------------------- | ------------------ | --------------------- |
| N=16 | 12.2M ops/s (+17% vs radix-2) | 16.5M ops/s        | **+54.6%** vs radix-2 |

### Experiment 5: Real FFT with Radix-4 (2026-01-21)

**Implementation**: Created `fft_real_radix4.wat` combining radix-4 complex FFT with real FFT post-processing.

**Result**: **SUCCESS** - All tests pass, provides faster rfft for power-of-4 N/2 sizes.

### Experiment 6: Automated Codelet Generator (2026-01-22)

**Hypothesis**: An automated codelet generator using DAG-based symbolic tracing with CSE would produce correct, optimized codelets for any size.

**Implementation**:

- Created `tools/codelet_generator.js` with:
  - Symbolic expression system (`Expr` class with hash-based CSE)
  - `StockhamTracer` that traces the FFT algorithm symbolically
  - `SIMDWATGenerator` that emits optimized WAT with SIMD v128 operations
  - Support for both radix-2 and radix-4 algorithms
- Generated N=32 (radix-2) and N=64 (radix-4) codelets
- Integrated into `fft_combined.wat` with dispatch logic

**Result**: **PARTIAL SUCCESS - Correct but limited performance gains**

| Size | Standalone codelet vs general loop | rfft benchmark vs fftw-js |
| ---- | ---------------------------------- | ------------------------- |
| N=32 | +15.6% faster                      | rfft(64): -32.0%          |
| N=64 | +18.9% faster                      | rfft(128): -33.4%         |

**Analysis**:

1. **Codelets are numerically correct** - Max error ~10⁻¹⁵ (machine precision)
2. **Too many locals cause register spilling**:
   - N=32 codelet: 320+ locals (32 inputs + 288 temps)
   - N=64 codelet: 768+ locals (64 inputs + 704 temps)
   - WebAssembly engines spill excess locals to stack memory
3. **General SIMD loop already efficient** - The Stockham loop with inlined SIMD complex multiply is competitive; codelet overhead (spills) may negate unrolling benefits
4. **Hand-written $fft_16 still wins** - Only ~20 locals, carefully optimized, used by rfft(32) which beats fftw-js by +43%

**Key Insight**: FFTW's approach of composing **small** codelets (N≤16) hierarchically is superior to generating **large** monolithic codelets. Large unrolled codelets exhaust registers and cause spills.

**Lesson**: Future work should focus on:

- Keeping codelets small (N≤16)
- Using hierarchical composition for larger sizes
- Register-aware scheduling to minimize live variables

**Files created/modified**:

- `tools/codelet_generator.js` - Automated codelet generator
- `modules/fft_combined.wat` - Added $fft_32, $fft_64 codelets and dispatch

### Experiment 7: Inline SIMD Complex Multiply (2026-01-22)

**Hypothesis**: The radix-2 Stockham code uses `call $simd_cmul` while radix-4 inlines the complex multiply. Inlining should eliminate function call overhead.

**Implementation**:

- Replaced `(call $simd_cmul (local.get $x1) (local.get $w))` with inline SIMD operations
- Used same pattern as radix-4: shuffle + multiply + sign flip

**Result**: **NO IMPROVEMENT - V8 already inlines small functions**

| Size (radix-2) | Before (function call) | After (inlined) | Change |
| -------------- | ---------------------- | --------------- | ------ |
| N=32           | 6.15M ops/s            | 6.15M ops/s     | ~0%    |
| N=128          | 1.59M ops/s            | 1.59M ops/s     | ~0%    |
| N=512          | 346K ops/s             | 346K ops/s      | ~0%    |

**Analysis**:

1. **V8's TurboFan JIT already inlines** small hot functions like `$simd_cmul`
2. **Code was already fused** - twiddle multiply and butterfly were in the same loop, no intermediate memory stores
3. **The optimization plan's description was outdated** - it described "separate passes" but our code never did that

**Key Insight**: The "twiddle-butterfly fusion" optimization (Priority A, expected +25-40%) doesn't apply to our implementation because we already have them fused. The description in the plan was based on a different code pattern.

**Lesson**: Modern JIT compilers are very good at inlining. Manual inlining for small functions rarely helps and reduces code readability.

### Experiment 8: Fused Real-FFT Codelets (2026-01-22)

**Hypothesis**: Creating specialized rfft codelets that fuse the FFT computation with hardcoded post-processing twiddles will eliminate memory loads and function call overhead.

**Implementation**:

1. **`$rfft_8`**: Fully fused codelet
   - Inline FFT-4 butterfly operations
   - Hardcoded post-processing twiddles (W_8^k constants)
   - Outputs: DC, Nyquist, X[1], X[2], X[3] with no memory loads for twiddles

2. **`$rfft_32`**: Hybrid approach
   - Calls existing `$fft_16` codelet (already optimized)
   - Hardcoded post-processing with inline twiddle constants
   - Processes k=1..7 pairs plus middle element with no twiddle memory loads

**Results**: **SIGNIFICANT IMPROVEMENT**

| Size | wat-fft before | wat-fft after | fftw-js (f32) | vs fftw-js  |
| ---- | -------------- | ------------- | ------------- | ----------- |
| N=8  | 20.5M ops/s    | 24.2M ops/s   | 10.8M ops/s   | **+123.8%** |
| N=32 | 12.0M ops/s    | 13.1M ops/s   | 9.0M ops/s    | **+45.7%**  |

**Analysis**:

1. **For N=8**, the fused codelet eliminates:
   - Function call to `$fft` (4-point FFT)
   - All twiddle memory loads in post-processing
   - Loop overhead in post-processing

2. **For N=32**, the hybrid approach:
   - Keeps the well-optimized `$fft_16` codelet (avoids code duplication)
   - Eliminates 7 twiddle memory loads per pair (14 total) + middle element

3. **Diminishing returns for larger sizes**:
   - FFT computation dominates for N≥64
   - Post-processing overhead becomes proportionally smaller
   - Full fusion would require too many locals (register pressure)

**Key Insight**: For small sizes (N≤32), the overhead of memory loads and function calls is proportionally significant. Fusing these operations with hardcoded constants provides substantial speedups without register pressure issues.

### Experiment 9: Hierarchical FFT Composition (2026-01-23)

**Hypothesis**: Building larger FFTs from smaller optimized codelets (hierarchical composition) would avoid the slow radix-2 Stockham path for non-power-of-4 sizes.

**Problem**: rfft(64) calls fft(32) internally, but 32 is NOT a power of 4 (2^5), so it was using radix-2 Stockham with 5 stages instead of the faster radix-4. Similarly, rfft(128) calls fft(64), and rfft(256) calls fft(128).

**Solution**: Use DIF (Decimation in Frequency) decomposition to build larger FFTs hierarchically:

1. **`$fft_32`**: Implemented using two `$fft_16_at` calls
   - First pass: 16 butterflies combining x[k] and x[k+16] with hardcoded W_32^k twiddles
   - Then two independent FFT-16 calls on each half

2. **`$fft_64`**: Implemented using two `$fft_32_at` calls
   - First pass: 32 butterflies combining x[k] and x[k+32] with hardcoded W_64^k twiddles
   - Then two independent FFT-32 calls on each half

3. **`$fft_128`**: Implemented using two `$fft_64_at` calls
   - First pass: 64 butterflies combining x[k] and x[k+64] with hardcoded W_128^k twiddles
   - Then two independent FFT-64 calls on each half

4. **`$fft_256`**: Implemented using two `$fft_128_at` calls
   - First pass: 128 butterflies combining x[k] and x[k+128] with hardcoded W_256^k twiddles
   - Then two independent FFT-128 calls on each half

5. **`$fft_512`**: Implemented using two `$fft_256_at` calls
   - First pass: 256 butterflies combining x[k] and x[k+256] with hardcoded W_512^k twiddles
   - Then two independent FFT-256 calls on each half

6. **`$fft_1024`**: Implemented using two `$fft_512_at` calls
   - First pass: 512 butterflies combining x[k] and x[k+512] with hardcoded W_1024^k twiddles
   - Then two independent FFT-512 calls on each half

7. **Parameterized codelets** (`$fft_16_at`, `$fft_32_at`, `$fft_64_at`, `$fft_128_at`, `$fft_256_at`, `$fft_512_at`): Take a base offset parameter to operate at arbitrary memory locations, enabling composition.

**Implementation Pattern** (DIF decomposition):

```wat
(func $fft_N
  ;; First pass: for each k, compute:
  ;;   first_half[k] = x[k] + x[k+N/2]
  ;;   second_half[k] = (x[k] - x[k+N/2]) * W_N^k

  ;; k=0: W_N^0 = (1, 0) - no multiply needed
  (local.set $a (v128.load (i32.const 0)))
  (local.set $b (v128.load (i32.const N/2*16)))  ;; offset = N/2 * 16 bytes
  (v128.store (i32.const 0) (f64x2.add (local.get $a) (local.get $b)))
  (v128.store (i32.const N/2*16) (f64x2.sub (local.get $a) (local.get $b)))

  ;; k=1..N/2-1: apply hardcoded W_N^k twiddles using SIMD complex multiply
  ;; ...

  ;; Then run FFT-N/2 on each half
  (call $fft_N/2_at (i32.const 0))
  (call $fft_N/2_at (i32.const N/2*16))
)
```

**Results**: **SIGNIFICANT IMPROVEMENT for N=64, N=128, N=256, N=512, N=1024, and N=2048**

| Size   | Before (radix-2)   | After (hierarchical) | vs fftw-js |
| ------ | ------------------ | -------------------- | ---------- |
| N=64   | 4.8M ops/s (-30%)  | 6.9M ops/s           | **+3.4%**  |
| N=128  | 2.9M ops/s (-33%)  | 3.5M ops/s           | **-16.8%** |
| N=256  | 1.2M ops/s (-17%)  | 1.7M ops/s           | **+12.3%** |
| N=512  | 0.7M ops/s (-21%)  | 0.76M ops/s          | **-15.8%** |
| N=1024 | 0.27M ops/s (-40%) | 0.34M ops/s          | **-26.9%** |
| N=2048 | 0.14M ops/s (-33%) | 0.14M ops/s          | **-31.1%** |

**Analysis**:

1. **N=64 now beats fftw-js**: By using hierarchical fft_32 (composed of two fft_16), we avoid the slow 5-stage radix-2 Stockham entirely
2. **N=128 improved by ~17 percentage points**: rfft(128) calls fft(64), which now uses hierarchical composition
3. **N=256 improved by ~30 percentage points**: rfft(256) calls fft(128), which now uses hierarchical composition - now **beats fftw-js by +12%**
4. **N=512 improved by ~5 percentage points**: rfft(512) calls fft(256), which now uses hierarchical $fft_256 composition
5. **N=1024 improved by ~13 percentage points**: rfft(1024) calls fft(512), which now uses hierarchical $fft_512 composition
6. **N=2048 improved by ~2 percentage points**: rfft(2048) calls fft(1024), which now uses hierarchical $fft_1024 composition
7. **Hardcoded twiddles**: All W_N^k twiddles are inline constants, eliminating memory loads
8. **Low register pressure**: Each sub-FFT codelet operates independently, avoiding register spills

**Key Insight**: FFTW's approach of using small codelets (N≤16) as building blocks works well in WebAssembly. The hierarchical composition avoids register pressure issues while still benefiting from fully-optimized small codelets.

**Files modified**:

- `modules/fft_real_combined.wat` - Added `$fft_16_at`, `$fft_32`, `$fft_32_at`, `$fft_64`, `$fft_64_at`, `$fft_128`, `$fft_128_at`, `$fft_256`, `$fft_256_at`, `$fft_512`, `$fft_512_at`, `$fft_1024`, updated `$fft` dispatch

### Final Performance Summary (2026-01-23)

After all optimizations, wat-fft Combined achieves:

**Complex FFT (vs fft.js pure JS):**
| Size | wat-fft Combined | fft.js (best JS) | Speedup |
| ------ | ---------------- | ---------------- | -------- |
| N=16 | 16.1M ops/s | 11.0M ops/s | **+46%** |
| N=64 | 3.8M ops/s | 2.7M ops/s | **+41%** |
| N=256 | 967K ops/s | 554K ops/s | **+75%** |
| N=1024 | 186K ops/s | 109K ops/s | **+71%** |
| N=4096 | 42.8K ops/s | 22.7K ops/s | **+89%** |

**Real FFT (vs fftw-js Emscripten/FFTW):**
| Size | wat-fft Combined | fftw-js (f32) | vs fftw-js |
| ----- | ---------------- | ------------- | ----------- |
| N=8 | 23.6M ops/s | 10.4M ops/s | **+126.1%** |
| N=16 | 12.1M ops/s | 10.0M ops/s | **+22.4%** |
| N=32 | 12.8M ops/s | 9.0M ops/s | **+43.3%** |
| N=64 | 6.9M ops/s | 6.6M ops/s | **+3.4%** |
| N=128 | 3.5M ops/s | 4.2M ops/s | -16.8% |
| N=256 | 1.7M ops/s | 1.5M ops/s | **+12.3%** |
| N=512 | 757K ops/s | 900K ops/s | -15.8% |
| N=1024 | 336K ops/s | 460K ops/s | -26.9% |
| N=2048 | 142K ops/s | 206K ops/s | -31.1% |
| N=4096 | 59K ops/s | 106K ops/s | -44.2% |

**Conclusion**:

- Fused rfft codelets (`$rfft_8`, `$rfft_32`) provide **massive speedups for small sizes**
- For N≤64 and N=256, we now **beat fftw-js** (Emscripten port of FFTW)
- Hierarchical FFT composition (`$fft_32`, `$fft_64`, `$fft_128`, `$fft_256`, `$fft_512`, `$fft_1024`) provides consistent speedups
- For N=128 and N≥512, fftw-js still has an advantage but the gap is narrowing

**Key findings from codelet generator experiment (2026-01-22)**:

- Automated codelet generation works and produces correct code
- However, large codelets (N≥32) have too many locals causing register spills
- Small hand-written codelets (N≤16) outperform generated large codelets
- Fused rfft codelets with hardcoded twiddles are very effective for small N

**Key findings from fused rfft experiment (2026-01-22)**:

- Fusing FFT + post-processing eliminates significant overhead for small N
- For N=8: eliminating all function calls and twiddle loads gives +123% vs fftw-js
- For N=32: hybrid approach (call fft_16 + hardcoded post-processing) gives +46% vs fftw-js
- Diminishing returns for N≥64 as FFT computation dominates

**Key findings from hierarchical composition experiment (2026-01-23)**:

- DIF decomposition with parameterized codelets (`$fft_16_at`, `$fft_32_at`, `$fft_64_at`, `$fft_128_at`, `$fft_256_at`, `$fft_512_at`) enables hierarchical composition
- `$fft_32` (2x fft_16), `$fft_64` (2x fft_32), `$fft_128` (2x fft_64), `$fft_256` (2x fft_128), `$fft_512` (2x fft_256), and `$fft_1024` (2x fft_512) avoid slow radix-2 path
- N=64 improved from -30% to **+3.4%** vs fftw-js
- N=128 improved from -33% to **-16.8%** vs fftw-js
- N=256 improved from -17% to **+12.3%** vs fftw-js (rfft(256) calls fft(128) internally)
- N=512 improved from -21% to **-15.8%** vs fftw-js (rfft(512) calls fft(256) internally)
- N=1024 improved from -40% to **-26.9%** vs fftw-js (rfft(1024) calls fft(512) internally)
- N=2048 improved from -33% to **-31.1%** vs fftw-js (rfft(2048) calls fft(1024) internally)

**Failed experiment: $fft_2048 hierarchical composition (2026-01-23)**:

Attempted to extend hierarchical composition to N=2048 to improve rfft(4096) performance. The implementation was correct but **made performance worse**:

- N=4096: went from **-44%** to **-51%** vs fftw-js (7 percentage points worse)

**Why $fft_2048 failed**:

| Factor        | Impact                                                           |
| ------------- | ---------------------------------------------------------------- |
| Code size     | 13,800 lines of WAT → instruction cache thrashing                |
| Call depth    | 8 levels deep: `$fft_2048` → `$fft_1024_at` → ... → `$fft_16_at` |
| Twiddle bloat | 1024 inline twiddle constants in just the first pass             |

**The crossover point**: Hierarchical composition is beneficial when:

- Small codelets fit in instruction cache
- Function call overhead < loop overhead saved
- Twiddle inline savings > code bloat cost

For N≥2048, the simple radix-2/radix-4 Stockham loops are more efficient because:

- Compact, cache-friendly code
- Modern CPUs predict simple loops well
- Twiddle table lookups are fast when data is hot in cache

**Conclusion**: The hierarchical approach has **diminishing returns** and becomes **counterproductive** at N≥2048. The optimal cutoff for hierarchical codelets is around N=1024.

Further gains for large N would require fundamentally different approaches:

- Cache-oblivious recursive algorithms (not just hierarchical codelets)
- Runtime planning (like FFTW's planner) to select algorithms per-size
- Depth-first recursion with explicit stack management to improve cache locality

### Experiment 10: Depth-First Recursive DIF FFT (2026-01-23)

**Hypothesis**: A depth-first recursive FFT using DIF (Decimation in Frequency) decomposition would improve cache locality for large N, as the working set stays hot in cache during recursion.

**Implementation** (`modules/fft_recursive.wat`):

- DIF decomposition: butterflies first, then recurse on halves
- No data reordering needed before recursion (unlike DIT)
- Base cases: N=2, 4, 8, 16 codelets with hardcoded twiddles
- Twiddle stride parameter to use precomputed twiddle table at different recursion levels
- Bit-reversal permutation at the end (DIF produces bit-reversed output)

**Result**: **SLOWER THAN EXPECTED** - Does not achieve the expected +15-25% improvement

| Size   | vs Combined (iterative) | Notes                            |
| ------ | ----------------------- | -------------------------------- |
| N=64   | **-37%**                | Function call overhead dominates |
| N=256  | **-53%**                |                                  |
| N=1024 | **-50%**                |                                  |
| N=2048 | **-39%**                |                                  |
| N=4096 | **-55%**                |                                  |
| N=8192 | **-11%**                | Gap narrows at larger sizes      |

**Why the depth-first approach is slower**:

| Factor                 | Impact                                                           |
| ---------------------- | ---------------------------------------------------------------- |
| Function call overhead | WASM function calls have non-trivial overhead                    |
| Bit-reversal overhead  | DIF produces bit-reversed output, requiring O(N) permutation     |
| Loop vs codelet        | Recursive DIF loop is slower than hardcoded codelets             |
| Twiddle table loads    | Each level loads twiddles from memory (vs hardcoded in codelets) |

**Why the gap narrows at N=8192**:

- Cache locality benefits start to show at very large N
- Working set exceeds L2 cache, so depth-first helps
- For N≥16384, depth-first might eventually beat iterative

**Attempted optimizations that did NOT help**:

1. **In-place bit-reversal**: Swap pairs directly instead of using secondary buffer → No improvement
2. **Iterative base case (N≤64)**: Use iterative DIF loop instead of codelets → Made it **worse** because codelets have hardcoded twiddles and no loop overhead

**Key insight**: The iterative Stockham algorithm avoids bit-reversal entirely through its ping-pong buffer structure, giving it a fundamental advantage over recursive DIF.

**Conclusion**: Priority D (Depth-First Recursive FFT) does **NOT** provide the expected +15-25% improvement for N≥1024. The overhead of recursive calls and bit-reversal outweighs the cache locality benefits for typical FFT sizes. The gap only narrows significantly at N≥8192.

**Files created**:

- `modules/fft_recursive.wat` - Depth-first recursive DIF FFT
- `tests/recursive.test.js` - Correctness tests
- `benchmarks/recursive.bench.js` - Performance benchmarks

### Experiment 11: SIMD Post-Processing for Real FFT (2026-01-24)

**Hypothesis**: The scalar post-processing loop in rfft can be optimized using SIMD operations to reduce instruction count and improve throughput.

**Background**: The rfft post-processing computes for each pair (k, n2-k):

- `sum = Z[k] + conj(Z[n2-k])`
- `diff = Z[k] - conj(Z[n2-k])`
- `wd = W_rot * diff` where `W_rot = (w_im, -w_re)` is a rotated twiddle
- `X[k] = 0.5 * (sum + wd)`

The original implementation used scalar f64 operations. Each pair required 8 f64 loads, 8 f64 stores, and ~20 f64 arithmetic operations.

**Implementation** (`$rfft_postprocess_simd`):

1. Added `$CONJ_MASK` global for sign-flipping imaginary parts
2. Created SIMD version that:
   - Loads Z[k] and Z[n2-k] as v128
   - Computes conj() using v128.xor with $CONJ_MASK
   - Uses SIMD f64x2 add/sub for sum and diff
   - Creates W_rot using shuffle + sign flip
   - Performs complex multiply using inline SIMD pattern
   - Stores results as v128
3. Modified `$rfft` to dispatch to SIMD for N >= 128

**Result**: **SUCCESS - 2-8 percentage point improvement across all sizes**

| Size   | Before (scalar) | After (SIMD) | Improvement |
| ------ | --------------- | ------------ | ----------- |
| N=128  | -16.8%          | -8.3%        | **+8.5pp**  |
| N=256  | +12.3%          | +19.0%       | **+6.7pp**  |
| N=512  | -15.8%          | -13.5%       | **+2.3pp**  |
| N=1024 | -26.9%          | -21.9%       | **+5.0pp**  |
| N=2048 | -31.1%          | -25.5%       | **+5.6pp**  |
| N=4096 | -44.2%          | -40.1%       | **+4.1pp**  |

**Analysis**:

1. **SIMD reduces instruction count**: v128 operations process 2 f64 values at once
2. **Fewer memory operations**: v128.load/store replaces pairs of f64.load/store
3. **Better instruction pipelining**: SIMD operations have good throughput on modern CPUs
4. **Consistent gains**: The improvement is relatively consistent (2-8pp) across all sizes

**Key insight**: The post-processing accounts for roughly 10-20% of total rfft time for larger sizes. SIMD optimization of this phase gives proportional improvement matching the Priority H expectation of "+5-10% for rfft".

**Conclusion**: Priority G/H (SIMD post-processing) provides meaningful but not dramatic improvements. The main bottleneck remains the FFT computation itself, not the post-processing.

**Files modified**:

- `modules/fft_real_combined.wat` - Added `$CONJ_MASK` global, `$rfft_postprocess_simd` function, modified `$rfft` dispatch

### Experiment 12: Relaxed SIMD FMA for Complex Multiply (2026-01-24)

**Hypothesis**: Using `f64x2.relaxed_madd` (fused multiply-add) instead of separate `f64x2.mul` + `f64x2.add` would reduce instruction count and improve performance by 5-15%.

**Implementation** (`modules/fft_real_combined_fma.wat`):

1. Modified `$simd_cmul` to use FMA:

   ```wat
   ;; Before: f64x2.add(f64x2.mul(...), f64x2.mul(...))
   ;; After:  f64x2.relaxed_madd(..., ..., f64x2.mul(...))
   ```

2. Updated 3 inline complex multiply patterns in `$rfft_postprocess_simd`

3. Saves 1 instruction per complex multiply operation

**Result**: **MODEST IMPROVEMENT - +1% to +5%**

| Size   | Standard    | FMA         | Speedup   |
| ------ | ----------- | ----------- | --------- |
| N=128  | 3.34M ops/s | 3.51M ops/s | **+4.9%** |
| N=256  | 1.75M ops/s | 1.73M ops/s | -1.1%     |
| N=512  | 799K ops/s  | 808K ops/s  | **+1.1%** |
| N=1024 | 364K ops/s  | 368K ops/s  | **+1.1%** |
| N=2048 | 161K ops/s  | 166K ops/s  | **+2.6%** |
| N=4096 | 62.8K ops/s | 63.9K ops/s | **+1.7%** |

**Analysis**:

1. **V8's JIT already optimizes well**: Modern JIT compilers can fuse multiply-add sequences automatically, reducing the benefit of explicit FMA instructions
2. **Limited scope of optimization**: FMA only applies to `$simd_cmul` function and `$rfft_postprocess_simd` inline patterns; the main FFT codelets (`$fft_16`, `$fft_32`, etc.) use different patterns
3. **Post-processing is a small fraction**: For larger N, the FFT computation dominates; post-processing accounts for only 10-20% of total time
4. **N=256 anomaly**: Consistently shows slight slowdown (-1.1%), possibly due to cache alignment or branch prediction changes

**Correctness**: ✅ Results are bit-identical to the standard version (max difference: 0)

**Key Insight**: The relaxed SIMD FMA optimization provides real but modest improvements. The expected +5-15% gain was overly optimistic because V8 already does a good job optimizing SIMD code. The optimization is still worthwhile for the slight improvement and better numerical precision (FMA eliminates intermediate rounding).

**Files created**:

- `modules/fft_real_combined_fma.wat` - FMA-optimized Real FFT module
- `benchmarks/rfft_fma.bench.js` - FMA vs standard benchmark

### Experiment 13: f32 Dual-Complex Real FFT (2026-01-24)

**Hypothesis**: Applying the f32 dual-complex optimization (+105% for complex FFT) to real FFT should close the performance gap with fftw-js.

**Discovery**: The existing `fft_real_f32.wat` uses the basic `fft_stockham_f32` module, not the optimized `fft_stockham_f32_dual.wat`. This meant the +105% dual-complex speedup was not being used for real FFT.

**Implementation** (`modules/fft_real_f32_dual.wat`):

1. Based on `fft_stockham_f32_dual.wat` (the +105% optimized version)
2. Added RFFT twiddle precomputation at offset 131072
3. Added rfft post-processing (scalar f32 for now)

**Result**: **SIGNIFICANT IMPROVEMENT**

| Size   | Dual        | Existing f32 | fftw-js     | vs Existing | vs fftw-js |
| ------ | ----------- | ------------ | ----------- | ----------- | ---------- |
| N=64   | 5.72M ops/s | 4.48M ops/s  | 6.79M ops/s | **+27.6%**  | -15.8%     |
| N=128  | 3.15M ops/s | 2.38M ops/s  | 4.14M ops/s | **+32.4%**  | -23.9%     |
| N=256  | 1.56M ops/s | 1.10M ops/s  | 1.44M ops/s | **+41.3%**  | **+7.9%**  |
| N=512  | 787K ops/s  | 500K ops/s   | 857K ops/s  | **+57.4%**  | -8.1%      |
| N=1024 | 385K ops/s  | 244K ops/s   | 465K ops/s  | **+57.7%**  | -17.3%     |
| N=2048 | 196K ops/s  | 115K ops/s   | 227K ops/s  | **+69.9%**  | -14.1%     |
| N=4096 | 91.5K ops/s | 52.9K ops/s  | 101K ops/s  | **+72.9%**  | -9.3%      |

**Analysis**:

1. **+28% to +73% speedup** over existing f32 rfft, scaling with size
2. **Beats fftw-js at N=256** (+7.9%) - first time beating fftw-js at this size!
3. **Gap with fftw-js reduced** from ~40% to just 8-24%
4. The dual-complex FFT provides most of the benefit; post-processing is not yet SIMD-optimized

**Future optimization**: Adding f32x4 SIMD to the post-processing loop could provide additional gains.

**Correctness**: ✅ Exact match with existing f32 rfft output for all tested sizes

**Files created**:

- `modules/fft_real_f32_dual.wat` - f32 dual-complex real FFT
- `benchmarks/rfft_f32_dual.bench.js` - benchmark script

### Experiment 14: f32x4 SIMD Post-Processing for f32 Dual-Complex RFFT (2026-01-24)

**Hypothesis**: The scalar f32 post-processing in `fft_real_f32_dual.wat` can be optimized using f32x4 SIMD to process 2 complex pairs per iteration, matching the dual-complex throughput of the FFT core.

**Background**: The f32 dual-complex rfft from Experiment 13 used scalar f32 post-processing, which was a bottleneck since the FFT core processes 2 complex numbers per SIMD operation.

**Implementation** (`$rfft_postprocess_simd`):

1. Added `$CONJ_MASK_F32` global: `v128.const i32x4 0 0x80000000 0 0x80000000` for sign-flipping imaginary parts
2. Created SIMD version that processes 2 pairs per iteration:
   - Loads `[Z[k], Z[k+1]]` contiguously as f32x4
   - Loads `[Z[n2-k-1], Z[n2-k]]` and shuffles to `[Z[n2-k], Z[n2-k-1]]` order
   - Computes conjugate using v128.xor with $CONJ_MASK_F32
   - Uses inline f32x4 complex multiply pattern
   - Handles remaining odd pair with 64-bit loads
   - Handles middle element for even n2
3. Modified `$rfft` to dispatch to SIMD for N >= 128

**Result**: **SIGNIFICANT IMPROVEMENT - +5 to +13 percentage points vs fftw-js**

| Size   | Before (scalar) | After (SIMD) | Improvement |
| ------ | --------------- | ------------ | ----------- |
| N=256  | +7.9%           | **+21.1%**   | **+13.2pp** |
| N=512  | -8.1%           | **+1.2%**    | **+9.3pp**  |
| N=1024 | -17.3%          | -4.3%        | **+13.0pp** |
| N=2048 | -14.1%          | -1.1%        | **+13.0pp** |
| N=4096 | -9.3%           | -3.8%        | **+5.5pp**  |

**Analysis**:

1. **f32x4 matches FFT core throughput**: The post-processing now processes 2 complex pairs per iteration, matching the dual-complex FFT core
2. **N=256 and N=512 now beat fftw-js**: These sizes now show positive margins (+21% and +1%)
3. **N=1024 to N=4096 nearly at parity**: Within 4% of fftw-js, down from 9-17% gap
4. **Diminishing returns at N=4096**: The FFT computation dominates at larger sizes

**Key insight**: The post-processing accounts for a larger fraction of total time in f32 than f64 (because f32 FFT is faster), so SIMD optimization provides proportionally larger gains.

**Conclusion**: f32x4 SIMD post-processing is highly effective for f32 dual-complex rfft. Combined with the dual-complex FFT core, we now match or beat fftw-js at N=256 and N=512, and are within 4% at all other sizes.

**Files modified**:

- `modules/fft_real_f32_dual.wat` - Added `$CONJ_MASK_F32` global, `$rfft_postprocess_simd` function, modified `$rfft` dispatch

### Updated Performance Summary (2026-01-24)

**Real FFT f32 Dual-Complex (vs fftw-js Emscripten/FFTW):**
| Size | wat-fft f32 Dual | fftw-js (f32) | vs fftw-js |
| ------ | ---------------- | ------------- | ----------- |
| N=64 | 5.68M ops/s | 6.82M ops/s | -16.7% |
| N=128 | 3.44M ops/s | 4.17M ops/s | -17.5% |
| N=256 | 1.80M ops/s | 1.48M ops/s | **+21.1%** |
| N=512 | 913K ops/s | 902K ops/s | **+1.2%** |
| N=1024 | 437K ops/s | 456K ops/s | -4.3% |
| N=2048 | 220K ops/s | 222K ops/s | -1.1% |
| N=4096 | 101K ops/s | 105K ops/s | -3.8% |

**Summary after f32x4 SIMD post-processing optimization**:

- **Beats fftw-js** at N=256 (+21%) and N=512 (+1%)
- **Nearly at parity** at N=1024 to N=4096 (within 4%)
- **Still behind** at N=64 and N=128 (-17%) where fftw-js's small-N codelets dominate
- The f32 dual-complex approach provides +26% to +97% speedup over the existing f32 rfft

### Experiment 15: Fully Fused FFT-64 Codelet (2026-01-24)

**Hypothesis**: The hierarchical FFT-64 (which calls fft_32 → fft_16) has function call overhead that can be eliminated by fully inlining all stages into a single monolithic function.

**Background**: For rfft(128), the internal FFT is N/2=64. The existing `$fft_64` function does:

1. 32 DIF butterflies with W_64^k twiddles (first stage)
2. Calls `$fft_32_at(0)` and `$fft_32_at(512)`
3. Each fft_32_at does 16 butterflies, then calls `$fft_16_at` twice

This results in 6 function calls per FFT-64: 2×fft_32 + 4×fft_16.

**Implementation** (`$fft_64_fused`):

1. Created code generator `scripts/generate_fused_fft64.js` that produces:
   - Stage 1: 32 DIF butterflies with W_64^k twiddles
   - Stage 2a: 16 DIF butterflies with W_32^k at offset 0
   - Stage 2b: 16 DIF butterflies with W_32^k at offset 512
   - Stage 3: Four inlined FFT-16 blocks at offsets 0, 256, 512, 768

2. Also created FMA version `scripts/generate_fused_fft64_fma.js` using `f64x2.relaxed_madd`

3. Updated dispatch in `$fft` to use `$fft_64_fused` when N=64

**Result**: **SIGNIFICANT IMPROVEMENT - N=128 gap halved**

| Metric           | Before    | After           | Improvement     |
| ---------------- | --------- | --------------- | --------------- |
| N=128 ops/sec    | 3,536,039 | 3,815,000       | **+7.9%**       |
| N=128 vs fftw-js | -17.4%    | **-8% to -11%** | **+6 to +9 pp** |

**Analysis**:

1. **Function call overhead eliminated**: Removed 6 function calls per FFT-64
2. **All twiddles hardcoded**: No memory loads for twiddle factors in stages 1-2
3. **Code size tradeoff**: ~1072 lines of WAT, acceptable for the performance gain
4. **Variance in benchmarks**: Results vary between -8% and -11% vs fftw-js depending on system load

**Key insight**: The hierarchical composition approach (Experiment 9) was already a big win, but the remaining gap at N=128 was partly due to function call overhead. Fully fusing eliminates this overhead.

**Conclusion**: The fused FFT-64 codelet roughly halves the performance gap at N=128. Combined with the f64 precision advantage (better numerical accuracy than fftw-js's f32), the -8% to -11% gap is acceptable.

**Files modified**:

- `modules/fft_real_combined.wat` - Added `$fft_64_fused`, updated dispatch
- `modules/fft_real_combined_fma.wat` - Added FMA version of `$fft_64_fused`
- `scripts/generate_fused_fft64.js` - Generator for non-FMA version
- `scripts/generate_fused_fft64_fma.js` - Generator for FMA version

---

## Future Optimization Opportunities (Research Summary)

The remaining performance gap at large sizes (N ≥ 512) is primarily due to **precision difference**: fftw-js uses f32 (4 values per SIMD) while we use f64 (2 values per SIMD). This section documents potential optimizations researched but not yet implemented.

### Priority I: f32 SIMD with Dual-Complex Processing ~~(Expected: +50-80%)~~ **IMPLEMENTED - UP TO +105%**

**Status**: ✅ Implemented and validated

**The Problem**: Our biggest competitive disadvantage:

- wat-fft: f64x2 = 2 doubles per v128 = 1 complex number per SIMD op
- fftw-js: f32x4 = 4 floats per v128 = 2 complex numbers per SIMD op

This 2x SIMD throughput difference, combined with 50% memory bandwidth reduction (8 bytes vs 16 bytes per complex), explains most of the 40% gap at N=4096.

**Solution Implemented**: `modules/fft_stockham_f32_dual.wat`

```wat
;; Process 2 complex f32 numbers per v128
;; Layout: [re0, im0, re1, im1] as f32x4
;; Pre-replicated twiddles: [w.re, w.im, w.re, w.im]
```

**Key Implementation Details**:

1. **Pre-replicated twiddles**: Stored as `[w.re, w.im, w.re, w.im]` (16 bytes each) at precompute time
2. **Separate code paths**: r<4 uses single-element processing, r>=4 uses dual-complex
3. **Correct Stockham addressing**: i1 = i0 + r_bytes (not n2_bytes) - this was the critical bug fix
4. **Inline dual-complex multiply**: No function call overhead

**Benchmark Results** (vs original f32 and fft.js):

| Size   | vs Original f32 | vs fft.js |
| ------ | --------------- | --------- |
| N=64   | +50.6%          | +64.1%    |
| N=256  | +74.7%          | +110.0%   |
| N=1024 | +92.1%          | +142.5%   |
| N=2048 | +95.9%          | +164.6%   |
| N=4096 | +104.8%         | +164.1%   |

**Why Experiment 1 Failed**: Our initial f32 dual-complex attempt was 15-20% _slower_ because:

1. Branch overhead: `if (r >= 2)` check in every iteration
2. Twiddle replication at runtime (extra shuffle per group)
3. Mixed r=1 and r>1 code paths hurt JIT optimization
4. **CRITICAL BUG**: Wrong butterfly partner offset (used n2_bytes instead of r_bytes)

**Files created**:

- `modules/fft_stockham_f32_dual.wat` - Main dual-complex FFT implementation
- `tests/fft_f32_dual.test.js` - Correctness tests
- `benchmarks/fft_f32_dual.bench.js` - Performance benchmarks

### Priority I-b: f32 Dual-Complex Real FFT ~~(Expected: +50-100%)~~ **IMPLEMENTED - +26% to +97%**

**Status**: ✅ Implemented and validated. See Experiments 13 and 14.

**Discovery (2026-01-24)**: The f32 dual-complex optimization (Priority I, +105%) was only applied to **complex FFT**, not to **real FFT**. The existing `fft_real_f32.wat` uses the basic `fft_stockham_f32` module, missing the dual-complex speedup.

**Solution Implemented**: Created `fft_real_f32_dual.wat` combining:

1. The dual-complex f32 Stockham FFT internally
2. f32x4 SIMD post-processing (added in Experiment 14)

**Benchmark Results** (after f32x4 SIMD post-processing):

| Size   | Dual        | Existing f32 | fftw-js     | vs Existing | vs fftw-js |
| ------ | ----------- | ------------ | ----------- | ----------- | ---------- |
| N=64   | 5.68M ops/s | 4.52M ops/s  | 6.82M ops/s | **+25.6%**  | -16.7%     |
| N=128  | 3.44M ops/s | 2.35M ops/s  | 4.17M ops/s | **+46.5%**  | -17.5%     |
| N=256  | 1.80M ops/s | 1.11M ops/s  | 1.48M ops/s | **+61.1%**  | **+21.1%** |
| N=512  | 913K ops/s  | 530K ops/s   | 902K ops/s  | **+72.2%**  | **+1.2%**  |
| N=1024 | 437K ops/s  | 245K ops/s   | 456K ops/s  | **+78.5%**  | -4.3%      |
| N=2048 | 220K ops/s  | 118K ops/s   | 222K ops/s  | **+87.0%**  | -1.1%      |
| N=4096 | 101K ops/s  | 51K ops/s    | 105K ops/s  | **+96.8%**  | -3.8%      |

**Key Results**:

- **+26% to +97% speedup** over existing f32 rfft
- **Beats fftw-js at N=256** (+21.1%) and **N=512** (+1.2%)
- **Nearly at parity** with fftw-js at N=1024 to N=4096 (within 4%)
- Correctness verified: exact match with existing f32 rfft output

**Files created**:

- `modules/fft_real_f32_dual.wat` - f32 dual-complex real FFT with SIMD post-processing
- `benchmarks/rfft_f32_dual.bench.js` - benchmark script

### Priority J: Relaxed SIMD FMA ~~(Expected: +5-15%)~~ **IMPLEMENTED - +1% to +5%**

**Status**: ✅ Implemented and validated. See Experiment 12.

**The Problem**: Complex multiply currently requires 4 multiplies + 2 adds:

```wat
;; Current: a*b = (ar*br - ai*bi, ar*bi + ai*br)
;; 4x f64.mul + 2x f64.add/sub
```

**Solution Implemented**: Use WebAssembly relaxed-simd FMA (fused multiply-add):

```wat
;; With FMA: 2x mul + 1x fma (saves 1 instruction per complex multiply)
(f64x2.relaxed_madd
  (v128.xor (local.get $ai) (global.get $SIGN_MASK))  ;; [-ai, ai]
  (local.get $bd)                                      ;; [bi, br]
  (f64x2.mul (local.get $ar) (local.get $b)))         ;; [ar*br, ar*bi]
```

**Benchmark Results** (vs standard SIMD):

| Size   | Standard    | FMA         | Speedup   |
| ------ | ----------- | ----------- | --------- |
| N=128  | 3.34M ops/s | 3.51M ops/s | **+4.9%** |
| N=256  | 1.75M ops/s | 1.73M ops/s | -1.1%     |
| N=512  | 799K ops/s  | 808K ops/s  | **+1.1%** |
| N=1024 | 364K ops/s  | 368K ops/s  | **+1.1%** |
| N=2048 | 161K ops/s  | 166K ops/s  | **+2.6%** |
| N=4096 | 62.8K ops/s | 63.9K ops/s | **+1.7%** |

**Why gains are smaller than expected**:

1. **V8's JIT already optimizes well**: Modern compilers may fuse mul+add sequences automatically
2. **Limited scope**: FMA only applies to `$simd_cmul` and post-processing; main FFT codelets don't use it
3. **Post-processing is a small fraction**: For larger N, the FFT computation dominates total time

**Correctness**: ✅ Results are bit-identical to the standard version

**Files created**:

- `modules/fft_real_combined_fma.wat` - FMA-optimized module
- `benchmarks/rfft_fma.bench.js` - Benchmark script

### Priority K: Split-Radix Algorithm (Expected: +6-10%)

**Status**: 🔬 Research complete, not implemented

**Description**: Split-radix is a hybrid radix-2/radix-4 algorithm that achieves the lowest proven arithmetic count for power-of-2 FFTs.

**How it works**:

- DFT(N) = DFT(N/2) of even elements + two DFT(N/4) of interleaved odd elements
- Radix-4 parts eliminate trivial twiddle multiplications (W^0 and W^{N/4})
- Modified split-radix (Johnson & Frigo, 2007) achieves ~6% fewer flops than standard

**Trade-offs**:

- More complex recursion pattern
- Multiple butterfly types
- We already have radix-4 SIMD, so gains are incremental
- At large sizes, memory bandwidth (not arithmetic) is the bottleneck

**Implementation complexity**: High
**Recommendation**: Lower priority than f32 SIMD. Consider only after f32 is implemented.

### Priority L: Conjugate-Pair Split-Radix (Expected: +5-15% for rfft)

**Status**: 🔬 Research complete, not implemented

**Description**: Groups twiddle factors W^k and W^{N-k} (complex conjugates) together, reducing memory bandwidth for twiddle loads by ~50%.

**How it works**:

- For real input, output has Hermitian symmetry: X[k] = conj(X[N-k])
- Process pairs (k, N-k) together, loading one twiddle for both
- A depth-first iterative variant (IEEE 2021) is cache-friendly and table-free

**Trade-offs**:

- We already have SIMD post-processing optimization
- Hierarchical codelets already use hardcoded twiddles up to N=1024
- Main benefit is for N > 1024 where twiddle loads dominate

**Implementation complexity**: Medium-High

### Priority M: Better Register Scheduling for Codelets (Expected: +10-20%)

**Status**: 🔬 Research complete, not implemented

**The Problem**: Experiment 6 (codelet generator) produced correct but slow code because:

- N=32 codelet: 320+ locals (32 inputs + 288 temps)
- N=64 codelet: 768+ locals → register spills to stack

FFTW's genfft solves this with optimal scheduling that minimizes simultaneous live values.

**Solution**: Improve codelet generator with Sethi-Ullman style scheduling:

1. Build dependency DAG of all operations
2. Number nodes by minimum required registers
3. Execute in order that keeps live values ≤ 16

**Implementation complexity**: High
**Recommendation**: High value for extending codelets beyond N=16, but requires significant codelet generator work.

### Priority N: Memory Alignment Hints (Expected: +0-5%)

**Status**: 🔬 Research complete, low priority

**Description**: Modern CPUs handle unaligned SIMD access well, but aligned hints may help JIT optimization.

**Implementation**:

```wat
;; Add alignment hint (currently omitted)
(v128.load align=16 (local.get $addr))
```

**Reality check**: Intel benchmarks show "no meaningful performance difference between aligned and unaligned instructions" for data access.

**Implementation complexity**: Low
**Recommendation**: Lowest priority, minimal expected gain.

### Not Applicable to Our Use Case

The following optimizations were researched but found **not applicable**:

| Optimization              | Why Not Applicable                                |
| ------------------------- | ------------------------------------------------- |
| **Bailey's 4-Step FFT**   | Only beneficial for N ≥ 1M (our max is N=4096)    |
| **Cache blocking/tiling** | N=4096 working set (64KB) fits in L2 cache        |
| **Depth-first recursion** | Already tried (Experiment 10), overhead > benefit |
| **Runtime planning**      | fftw-js uses fixed plans, not dynamic selection   |

### Optimization Priority Matrix

| Priority | Optimization          | Expected Gain | Actual Gain | Effort | Status                |
| -------- | --------------------- | ------------- | ----------- | ------ | --------------------- |
| **I**    | f32 SIMD dual-complex | +50-80%       | **+105%**   | High   | ✅ Done (complex FFT) |
| **I-b**  | f32 dual-complex rfft | +50-100%      | **+28-73%** | Medium | ✅ Done               |
| **J**    | Relaxed SIMD FMA      | +5-15%        | **+1-5%**   | Low    | ✅ Done               |
| **M**    | Register scheduling   | +10-20%       | -           | High   | 🔬 Research           |
| **K**    | Split-radix           | +6-10%        | -           | High   | 🔬 Research           |
| **L**    | Conjugate-pair        | +5-15%        | -           | Medium | 🔬 Research           |
| **N**    | Alignment hints       | +0-5%         | -           | Low    | 🔬 Research           |

### Key Insight

The **40% gap at N=4096** is explained by:

1. **Precision difference** (~2x): f32x4 vs f64x2 SIMD throughput
2. **Memory bandwidth** (~50%): 8 bytes vs 16 bytes per complex number
3. **FFTW's genfft codelets**: Better register scheduling, more CSE

If matching fftw-js performance is the goal, **Priority I (f32 SIMD)** is the critical path. All other optimizations are secondary.

---

## Phase 1: Testing & Benchmarking Infrastructure

### 1.1 Correctness Test Suite

Before optimizing, we need robust correctness tests that will catch regressions.

**Tests to create:**

- [ ] Property-based tests for all FFT sizes (2-8192)
- [ ] Round-trip tests: `ifft(fft(x)) ≈ x`
- [ ] Parseval's theorem: `sum(|x|²) = sum(|X|²)/N`
- [ ] Known-value tests (impulse, sine waves, DC)
- [ ] Linearity: `fft(ax + by) = a*fft(x) + b*fft(y)`
- [ ] Shift theorem: time shift = phase rotation in frequency
- [ ] Comparison against reference implementation (fft.js)

**File:** `tests/fft.correctness.test.js`

### 1.2 Performance Regression Suite

Automated benchmarks that run on every change.

**Metrics to track:**

- ops/sec for each size (64, 256, 1024, 4096)
- Memory bandwidth utilization
- Cache miss rates (if measurable)
- Comparison vs baseline and competitors

**File:** `benchmarks/regression.bench.js`

### 1.3 Profiling Tools

**Tools to create:**

- [ ] Instruction count analyzer (count WASM ops)
- [ ] Memory access pattern visualizer
- [ ] Twiddle factor reuse analyzer
- [ ] Butterfly operation counter

**File:** `tools/profiler.js`

---

## Phase 2: Codelet Generation System

### 2.1 Codelet Generator

Create a tool that generates optimal WAT code for small FFT sizes.

**Approach:**

```
Input: FFT size N (2, 4, 8, 16, 32, 64)
Output: Optimized WAT function for that size
```

**Optimizations to apply:**

1. Eliminate all loops (fully unrolled)
2. Precompute all twiddle factors as constants
3. Minimize temporary variables
4. Reorder operations for instruction pipelining
5. Use FMA where beneficial

**Example output for N=8:**

```wat
(func $fft8 (param $base i32)
  ;; All 8 inputs loaded, all butterflies unrolled
  ;; Twiddles are inline constants
  ;; ~56 adds, ~24 muls for complex 8-point FFT
)
```

**File:** `tools/codelet_generator.js`

### 2.2 Codelet Sizes to Generate

| Size | Radix | Multiplications | Additions | Priority |
| ---- | ----- | --------------- | --------- | -------- |
| 2    | 2     | 0               | 4         | High     |
| 4    | 4     | 0               | 16        | High     |
| 8    | 2×4   | 4               | 52        | High     |
| 16   | 4×4   | 24              | 148       | High     |
| 32   | 2×16  | 88              | 388       | Medium   |
| 64   | 4×16  | 264             | 964       | Medium   |

### 2.3 Codelet Verification

Each generated codelet must pass:

- [ ] Correctness test against reference
- [ ] Performance test (must beat generic loop)
- [ ] Code size check (not too large)

**File:** `tests/codelet.test.js`

---

## Phase 3: Algorithm Improvements

### 3.1 Split-Radix Algorithm

Current: Pure radix-2 (1 butterfly type)
Target: Split-radix (mixed radix-2 and radix-4)

**Benefits:**

- ~33% fewer multiplications than radix-2
- Better instruction-level parallelism

**Implementation steps:**

1. [ ] Implement radix-4 butterfly
2. [ ] Implement split-radix decomposition
3. [ ] Create hybrid that uses radix-4 when N is divisible by 4

**Theoretical improvement:** ~20% faster

### 3.2 Radix-4 Stockham

Pure radix-4 for power-of-4 sizes (4, 16, 64, 256, 1024, 4096).

**Benefits:**

- Twiddles W_N^0 = 1, W_N^(N/4) = -i are trivial
- 25% fewer stages than radix-2
- Better for SIMD (4-way operations)

**File:** `modules/fft_stockham_radix4.wat`

### 3.3 Mixed-Radix Support

For sizes like 12, 24, 48 (products of 2, 3, 4):

- Radix-2 kernel
- Radix-3 kernel
- Radix-4 kernel
- Combine hierarchically

---

## Phase 4: Memory Optimization

### 4.1 Cache-Oblivious Recursion

Instead of iterative stages, use recursive decomposition that naturally fits cache.

```
fft(x, n):
  if n <= CACHE_THRESHOLD:
    use_codelet(x, n)
  else:
    fft(even, n/2)
    fft(odd, n/2)
    combine(even, odd, n)
```

### 4.2 Twiddle Factor Optimization

Current: Precompute all N/2 twiddles
Better:

- Sizes ≤64: inline constants in codelets
- Sizes >64: compute on-the-fly with recurrence

**Twiddle recurrence:**

```
W[k+1] = W[k] * W[1]  (one complex multiply)
```

### 4.3 In-Place vs Out-of-Place

Analyze when in-place (Gentleman-Sande DIF) is better than out-of-place (Stockham).

---

## Phase 5: SIMD Deep Optimization

### 5.1 f32x4 Dual-Complex Operations

Pack 2 complex f32 numbers per v128 register.

**Current:** 1 complex per SIMD op
**Target:** 2 complex per SIMD op (requires algorithm restructuring)

### 5.2 Radix-4 SIMD Butterfly

A radix-4 butterfly naturally processes 4 values, perfect for f32x4.

```wat
;; Process 4 complex values in 2 v128 registers
;; Input: [x0, x1] [x2, x3] as v128 pairs
;; Output: [X0, X1] [X2, X3] with full SIMD utilization
```

### 5.3 Memory Coalescing

Ensure consecutive memory accesses for SIMD loads/stores.

---

## Tooling To Build

### Tool 1: Codelet Generator (`tools/codelet_generator.js`)

```javascript
// Usage: node tools/codelet_generator.js --size 8 --radix 2 --output modules/codelets/
// Generates: fft8.wat with fully unrolled, optimized code

class CodeletGenerator {
  generateFFT(size, options) { ... }
  optimizeForFMA(ast) { ... }
  reorderForPipelining(ast) { ... }
  emitWAT(ast) { ... }
}
```

### Tool 2: Performance Comparator (`tools/perf_compare.js`)

```javascript
// Usage: node tools/perf_compare.js --baseline main --candidate feature-branch
// Output: Performance diff table with statistical significance

async function comparePerformance(baseline, candidate, sizes) {
  // Run both, compute confidence intervals
  // Report: "N=1024: +15% ± 2% (p < 0.01)"
}
```

### Tool 3: Operation Counter (`tools/op_counter.js`)

```javascript
// Usage: node tools/op_counter.js modules/fft_stockham.wat
// Output:
//   f64.mul: 1,234
//   f64.add: 2,345
//   v128.load: 567
//   Total ops per butterfly: 12.3

function countOperations(watFile) {
  // Parse WAT, count by opcode type
}
```

### Tool 4: Memory Access Analyzer (`tools/mem_analyzer.js`)

```javascript
// Instrument WASM to log all memory accesses
// Visualize access patterns, detect cache-unfriendly patterns
```

---

## Test Suite Structure

```
tests/
├── correctness/
│   ├── fft.roundtrip.test.js      # ifft(fft(x)) = x
│   ├── fft.parseval.test.js       # Energy preservation
│   ├── fft.linearity.test.js      # Linearity property
│   ├── fft.shift.test.js          # Shift theorem
│   └── fft.reference.test.js      # Compare to fft.js
├── codelets/
│   ├── codelet.n2.test.js         # 2-point correctness
│   ├── codelet.n4.test.js         # 4-point correctness
│   ├── codelet.n8.test.js         # 8-point correctness
│   └── ...
├── performance/
│   ├── perf.regression.test.js    # Must not regress
│   ├── perf.scaling.test.js       # O(N log N) verification
│   └── perf.memory.test.js        # Memory bandwidth
└── integration/
    ├── real_fft.test.js           # Real FFT specific
    └── streaming.test.js          # Repeated FFT calls
```

---

## Migration Checklist

For each optimization, follow this checklist:

### Pre-Implementation

- [ ] Write correctness tests for affected code paths
- [ ] Establish baseline performance numbers
- [ ] Document expected improvement (with citation if applicable)

### Implementation

- [ ] Implement in isolated module
- [ ] Run correctness tests
- [ ] Run performance benchmarks
- [ ] Compare operation counts

### Post-Implementation

- [ ] All tests pass
- [ ] Performance improved (or explain why not)
- [ ] Code reviewed
- [ ] Documentation updated
- [ ] Benchmark results recorded

---

## Expected Performance Gains (Revised)

### High-Impact Optimizations (from FFTW Analysis)

| Optimization                 | Expected Gain    | Effort | Priority |
| ---------------------------- | ---------------- | ------ | -------- |
| **Twiddle-butterfly fusion** | **+25-40%**      | Medium | **A**    |
| **Fused real-FFT codelets**  | **+20-30% rfft** | Medium | **B**    |
| **DAG-based codelet gen**    | **+10-20%**      | High   | **C**    |
| **Depth-first recursion**    | **+15-25% N≥1K** | Medium | **D**    |
| Register-aware scheduling    | +5-10%           | Medium | E        |

### Original Optimizations

| Optimization      | Expected Gain   | Effort | Priority |
| ----------------- | --------------- | ------ | -------- |
| N=8 codelet       | +15% for N≤64   | Low    | 1        |
| N=16 codelet      | +10% for N≤256  | Low    | 2        |
| Radix-4 Stockham  | +20% overall    | Medium | 3        |
| Split-radix       | +25% overall    | High   | 4        |
| SIMD dual-complex | +30% for f32    | High   | 5        |
| Cache-oblivious   | +10% for N>1024 | Medium | 6        |

### Recommended Implementation Order

To close the 2x gap with fftw-js most efficiently:

1. **Twiddle-butterfly fusion** (Priority A) - Single biggest win, medium effort
2. **Basic codelets** (N=8, N=16) - Low-hanging fruit while building toward fusion
3. **Depth-first recursion** (Priority D) - Large-N improvement
4. **Fused rfft codelets** (Priority B) - Directly targets rfft performance
5. **Radix-4** - Compounds with above optimizations
6. **DAG codelet generator** (Priority C) - Long-term maintainability

**Target:** Match or exceed FFTW-js performance within 10%.

---

## References

1. FFTW Paper: ["The Design and Implementation of FFTW3"](https://www.fftw.org/fftw-paper-ieee.pdf) (Frigo & Johnson, 2005)
2. Genfft Compiler: ["A Fast Fourier Transform Compiler"](https://www.fftw.org/fftw-paper.pdf) (Frigo, PLDI 1999) - Won Most Influential Paper award
3. FFTW Adaptive Architecture: ["FFTW: An Adaptive Software Architecture"](https://www.fftw.org/fftw-paper-icassp.pdf) (Frigo & Johnson, ICASSP)
4. Implementing FFTs: ["Implementing FFTs in Practice"](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Implementing_FFTs_in_Practice.pdf) (S.G. Johnson)
5. SIMD FFT: ["A Portable Short Vector Version of FFTW"](https://users.ece.cmu.edu/~franzf/papers/mathmod.pdf) (Franchetti et al.)
6. Generating Kernels: ["Generating Small FFT Kernels"](<https://eng.libretexts.org/Bookshelves/Electrical_Engineering/Signal_Processing_and_Modeling/Fast_Fourier_Transforms_(Burrus)/10:_Implementing_FFTs_in_Practice/10.06:_Generating_Small_FFT_Kernels>) (Engineering LibreTexts)
7. Split-Radix: "On Computing the Split-Radix FFT" (Sorensen et al., 1986)
8. Stockham: "High-Speed Convolution and Correlation" (Stockham, 1966)
9. fftw-js: [GitHub Repository](https://github.com/j-funk/fftw-js) - FFTW compiled to JS/WASM via Emscripten

---

## Next Steps

1. **Immediate:** Create `tests/correctness/` test suite
2. **Week 1:** Build codelet generator, generate N=8,16 codelets
3. **Week 2:** Implement radix-4 Stockham
4. **Week 3:** Benchmark and iterate
