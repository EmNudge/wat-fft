# FFT Optimization Plan: Closing the Gap with FFTW

## Executive Summary

FFTW-js is ~2x faster than our implementation due to:

1. **Pre-optimized codelets** for sizes 2-64
2. **Hierarchical decomposition** using optimal codelet sizes
3. **FMA (fused multiply-add)** reducing instruction count
4. **Cache-aware memory access** patterns

This document outlines a phased approach to implement these optimizations with proper tooling and testing infrastructure.

---

## Why FFTW-js is Fast: Deep Analysis

FFTW-js is FFTW compiled to WebAssembly via Emscripten, meaning it inherits ALL of FFTW's sophisticated optimizations. Understanding these is key to closing the performance gap.

### 1. The genfft Codelet Generator

FFTW's codelets aren't just unrolled loops - they're generated by a compiler (`genfft`) that:

1. **DAG Representation**: Builds a directed acyclic graph of all operations
2. **Algebraic Simplification**: Applies constant folding, strength reduction, and algebraic identities
3. **Common Subexpression Elimination (CSE)**: Identifies and reuses repeated computations
4. **Network Transposition**: Transposes the computation graph, simplifies, transposes back - this exposes additional common subexpressions invisible to standard CSE
5. **Optimal Scheduling**: Produces a topological sort optimized for register allocation using cache-oblivious theory

**Key insight**: Their codelets have ~10-20% fewer operations than naively unrolled code due to these algebraic optimizations.

### 2. Operation Fusion (Twiddle-Butterfly Fusion)

**What we do**: Separate twiddle multiplication pass + butterfly pass

```
for each k: temp[k] = x[k] * twiddle[k]    // Load twiddle, load x, store temp
for each k: butterfly(temp[k], ...)        // Load temp again
```

**What FFTW does**: Fused twiddle-butterfly kernels

```
for each k: butterfly(x[k] * twiddle[k], ...)  // Load once, compute, store once
```

**Impact**: Eliminates N/2 memory round-trips per stage. For large FFTs, this is huge since memory bandwidth is often the bottleneck.

### 3. Automatic Real-FFT Specialization

FFTW's genfft **automatically derives** optimized real-FFT codelets from complex algorithms by exploiting conjugate symmetry properties. It doesn't just wrap a complex FFT - it generates specialized code that:

- Avoids redundant computation of conjugate pairs
- Uses real-only arithmetic where possible
- Fuses the pack/unpack steps with the FFT computation

### 4. Cache-Oblivious Recursion vs Our Iterative Approach

**Our Stockham**: Iterative, breadth-first - processes all butterflies at each stage before moving to the next

- Good for small FFTs that fit in cache
- Poor locality for large FFTs - data is touched log₂(N) times

**FFTW's approach**: Can use depth-first recursion

- Process subtrees completely before moving on
- Better cache utilization for large FFTs
- Working set stays in cache longer

### 5. The Planner (Less Relevant for WASM)

FFTW's runtime planner measures and selects algorithms. Since fftw-js is pre-compiled, it likely uses fixed plans - but those plans were optimized for the compilation target.

---

## Highest-Impact Optimizations (New Priorities)

Based on the FFTW analysis, these are the optimizations most likely to close the 2x performance gap:

### Priority A: Twiddle-Butterfly Fusion (Expected: +25-40%)

**Current problem**: Our Stockham FFT does separate passes:

1. Apply twiddle factors to half the data
2. Execute butterflies

**Solution**: Fuse twiddle multiplication directly into the butterfly operation.

```wat
;; CURRENT (two passes, extra memory traffic)
;; Pass 1: Apply twiddles
(f64.store (local.get $odd_re)
  (f64.sub
    (f64.mul (local.get $xo_re) (local.get $tw_re))
    (f64.mul (local.get $xo_im) (local.get $tw_im))))
;; Pass 2: Butterfly (must reload)

;; FUSED (single pass)
;; Load xo, load twiddle, compute twiddle*xo, butterfly with xe, store results
;; Never stores intermediate twiddle*xo result
```

**Implementation**:

1. Modify `fft_stockham.wat` butterfly loop to inline twiddle multiplication
2. Remove separate twiddle application pass
3. Adjust memory access pattern to interleave even/odd reads

**File**: Modify `modules/fft_stockham.wat`, `modules/fft_stockham_f32.wat`

### Priority B: Fused Real-FFT Codelets (Expected: +20-30% for rfft)

**Current problem**: Our real FFT is a wrapper:

1. Pack real data as complex
2. Run N/2 complex FFT
3. Post-process to extract real spectrum

**Solution**: Generate specialized real-FFT codelets that fuse pack + FFT + unpack.

```wat
;; CURRENT: 3 separate phases with intermediate storage

;; FUSED: For small N (8, 16, 32), generate monolithic rfft codelets
;; that directly transform real input to half-complex output
;; No intermediate complex representation needed
```

**Implementation**:

1. Create `tools/rfft_codelet_generator.js`
2. Generate fused codelets for N=8, 16, 32, 64
3. Use these as base cases in hierarchical decomposition

### Priority C: DAG-Based Codelet Optimization (Expected: +10-20% for codelets)

**Current approach**: Manually write or naively unroll FFT code

**Solution**: Build a simple codelet generator that:

1. Constructs operation DAG from FFT algorithm
2. Applies CSE (common subexpression elimination)
3. Schedules for register pressure
4. Emits optimized WAT

**Key CSE opportunities in FFT**:

- Twiddle factors: `W_N^k` and `W_N^{N-k}` are conjugates
- Butterfly symmetry: `a+b` and `a-b` share the same inputs
- Real-data symmetry: Exploits conjugate pairs

**Implementation**:

```javascript
// tools/codelet_generator.js
class FFTCodeletGenerator {
  buildDAG(size, algorithm) {
    /* DIT or DIF Cooley-Tukey */
  }
  applyCSE(dag) {
    /* Hash-based common subexpression elimination */
  }
  schedule(dag) {
    /* Topological sort minimizing live variables */
  }
  emitWAT(scheduled) {
    /* Generate WAT with local variables */
  }
}
```

### Priority D: Depth-First Recursive FFT (Expected: +15-25% for N ≥ 1024)

**Current approach**: Iterative Stockham (breadth-first)

- Processes all N/2 butterflies at stage 1, then all at stage 2, etc.
- For N=4096: Touches all 4096 elements 12 times (log₂N stages)

**Solution**: Recursive decomposition with codelet base cases

```
fft(x, N):
  if N <= 64:
    codelet_N(x)  // Fully unrolled, fits in registers
  else:
    fft(x_even, N/2)     // Complete left subtree
    fft(x_odd, N/2)      // Complete right subtree
    combine(x, N)        // Single pass butterfly
```

**Benefits**:

- Working set of N/2 stays hot in cache during recursion
- Better temporal locality
- Naturally composable with codelets

**Challenges**:

- WebAssembly call overhead (mitigate with larger codelet base cases)
- Stack usage (mitigate with explicit stack or tail calls)

**Implementation**:

1. Create `modules/fft_recursive.wat`
2. Use codelets for N ≤ 64
3. Benchmark crossover point where recursive beats iterative

### Priority E: Register-Aware Scheduling (Expected: +5-10%)

**Problem**: WASM has unlimited locals, but V8/SpiderMonkey map to limited registers. Poor scheduling causes register spills.

**Solution**: Schedule operations to minimize live variables at any point.

**Heuristic** (from FFTW paper):

1. Build dependency DAG
2. Use "Sethi-Ullman" style numbering
3. Execute operations in order that minimizes max simultaneous live values

**Target**: Keep live values ≤ 16 (typical register file size)

---

## Optimization Experiment Log

### Experiment 1: Dual-Complex f32 SIMD (2026-01-21)

**Hypothesis**: Process 2 f32 complex numbers per v128 register instead of 1, doubling SIMD throughput.

**Implementation**:

- Added `$simd_cmul_f32_dual` function for dual-complex multiply
- Modified butterfly loop to process pairs when r >= 2
- Added twiddle replication: `[w.re, w.im]` → `[w.re, w.im, w.re, w.im]`

**Result**: **FAILURE - 15-20% SLOWER**

| Size   | Before     | After      | Change |
| ------ | ---------- | ---------- | ------ |
| N=64   | 4.6M ops/s | 3.9M ops/s | -15%   |
| N=256  | 1.1M ops/s | 0.9M ops/s | -20%   |
| N=1024 | 249K ops/s | 203K ops/s | -18%   |
| N=4096 | 55K ops/s  | 46K ops/s  | -15%   |

**Analysis**: The overhead outweighed the benefits:

1. **Branch overhead**: `if (r >= 2)` check added to every group iteration
2. **Twiddle replication**: Extra shuffle instruction per group
3. **JIT interference**: More complex control flow may have prevented V8 optimizations
4. **Most work in r=1 stages**: For later stages (where l is large), r=1 and dual-complex doesn't help

**Lesson**: Simple, predictable loops optimize better than clever branching. The JIT compiler is already good at vectorization when the loop is simple.

### Experiment 2: N=8 Codelet (2026-01-21)

**Hypothesis**: Fully unrolled N=8 kernel with inline twiddles eliminates loop overhead and twiddle lookups.

**Implementation**: Attempted 3-stage unrolled kernel with hardcoded W_8^k twiddles.

**Result**: **FAILURE - Incorrect output**

**Analysis**: The Stockham FFT has complex permutation semantics:

1. Each stage reorders data differently than standard Cooley-Tukey DIT
2. The ping-pong buffer swap changes which indices map to which variables
3. Output positions after each stage are non-intuitive

**Lesson**: Codelet generation should be automated, not hand-written. A codelet generator that traces the actual algorithm would avoid these errors. This is exactly why FFTW uses `genfft`.

### Experiment 3: Radix-4 Stockham with SIMD (2026-01-21)

**Hypothesis**: Radix-4 algorithm has 50% fewer stages than radix-2 (log₄(N) vs log₂(N)), reducing memory passes. Combined with SIMD, should significantly improve throughput.

**Implementation**:

- Created `modules/fft_radix4.wat` with radix-4 Stockham algorithm
- SIMD N=4 kernel using v128 for butterfly operations
- Inlined SIMD complex multiply in the main butterfly loop
- Twiddles loaded as v128 [re, im] pairs
- Butterfly outputs computed with SIMD add/sub and shuffle for ±j multiplication

**Result**: **SUCCESS - Up to +51.3% faster than radix-2**

| Size   | Radix-2 (ops/s) | Radix-4 SIMD (ops/s) | Speedup                |
| ------ | --------------- | -------------------- | ---------------------- |
| N=4    | 24.3M           | 21.4M                | -11.9% (SIMD overhead) |
| N=16   | 10.4M           | 12.2M                | **+16.9%**             |
| N=64   | 3.5M            | 3.9M                 | **+10.5%**             |
| N=256  | 797K            | 977K                 | **+22.6%**             |
| N=1024 | 169K            | 196K                 | **+16.3%**             |
| N=4096 | 29.5K           | 44.6K                | **+51.3%**             |

**Analysis**:

1. **Stage reduction pays off**: log₄(4096) = 6 stages vs log₂(4096) = 12 stages
2. **SIMD inlining critical**: Initial version with function call overhead for `$simd_cmul` was 25-40% _slower_ than radix-2
3. **Large sizes benefit most**: Cache pressure reduction from fewer stages has compounding effect
4. **N=4 slower**: Fixed SIMD overhead dominates at tiny sizes; could special-case with scalar

**Key code pattern** (inlined SIMD complex multiply):

```wat
;; b1 = b * w1 (inlined, no function call)
(local.set $b1
  (f64x2.add
    (f64x2.mul (local.get $b)
      (i8x16.shuffle 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 (local.get $w1) (local.get $w1)))
    (f64x2.mul
      (f64x2.mul
        (i8x16.shuffle 8 9 10 11 12 13 14 15 0 1 2 3 4 5 6 7 (local.get $b) (local.get $b))
        (i8x16.shuffle 8 9 10 11 12 13 14 15 8 9 10 11 12 13 14 15 (local.get $w1) (local.get $w1)))
      (v128.const f64x2 -1.0 1.0))))
```

**Files created**:

- `modules/fft_radix4.wat` - Radix-4 Stockham with SIMD
- `tests/radix4.test.js` - Correctness tests
- `benchmarks/radix4.bench.js` - Performance benchmarks

### Current Performance (Final Benchmark Results)

Comparison against fft.js (best pure JS library, Radix-4 by Fedor Indutny):

| Size   | wat-fft Radix-2 | wat-fft Radix-4 | fft.js      | Radix-4 vs fft.js |
| ------ | --------------- | --------------- | ----------- | ----------------- |
| N=16   | 11.1M ops/s     | **12.2M ops/s** | 11.4M ops/s | **+7%**           |
| N=64   | 3.4M ops/s      | **3.8M ops/s**  | 2.8M ops/s  | **+37%**          |
| N=256  | 791K ops/s      | **971K ops/s**  | 561K ops/s  | **+73%**          |
| N=1024 | 166K ops/s      | **195K ops/s**  | 113K ops/s  | **+72%**          |
| N=4096 | 29.6K ops/s     | **44K ops/s**   | 23.5K ops/s | **+87%**          |

**Result**: Our Radix-4 SIMD implementation is now the fastest FFT across all tested sizes, outperforming even the best pure JS implementations by 7-87% depending on size.

Note: The original comparison target was fftw-js (FFTW via Emscripten). The numbers above use fft.js as the JS baseline. FFTW-js would likely still be faster at larger sizes due to its sophisticated codelet system and cache-oblivious algorithms.

### Experiment 4: N=16 Fully Unrolled Codelet (2026-01-21)

**Hypothesis**: Fully unrolled N=16 codelet with inline twiddle constants eliminates all loop overhead and twiddle lookups.

**Implementation**:

- Added `$fft_16` function with all 16 loads, 2 radix-4 stages fully unrolled
- Inline twiddle constants: W_16^1 through W_16^9
- No loops, no twiddle table lookups

**Result**: **SUCCESS - +54.6% speedup at N=16**

| Size | Before N=16 Codelet           | After N=16 Codelet | Improvement           |
| ---- | ----------------------------- | ------------------ | --------------------- |
| N=16 | 12.2M ops/s (+17% vs radix-2) | 16.5M ops/s        | **+54.6%** vs radix-2 |

### Experiment 5: Real FFT with Radix-4 (2026-01-21)

**Implementation**: Created `fft_real_radix4.wat` combining radix-4 complex FFT with real FFT post-processing.

**Result**: **SUCCESS** - All tests pass, provides faster rfft for power-of-4 N/2 sizes.

### Final Performance Summary (2026-01-21)

After all optimizations, wat-fft Radix-4 achieves:

| Size   | wat-fft Radix-4 | fft.js (best JS) | Speedup  |
| ------ | --------------- | ---------------- | -------- |
| N=16   | 16.1M ops/s     | 11.0M ops/s      | **+46%** |
| N=64   | 3.8M ops/s      | 2.7M ops/s       | **+41%** |
| N=256  | 967K ops/s      | 554K ops/s       | **+75%** |
| N=1024 | 186K ops/s      | 109K ops/s       | **+71%** |
| N=4096 | 42.8K ops/s     | 22.7K ops/s      | **+89%** |

**Conclusion**: The radix-4 approach with SIMD and codelets successfully achieves significant speedups over the best pure JS implementation. Further gains would require:

- Codelet generator for automated N=32, N=64 generation
- Depth-first recursion for better cache locality at N≥8192
- FFTW-style planner for runtime algorithm selection

---

## Phase 1: Testing & Benchmarking Infrastructure

### 1.1 Correctness Test Suite

Before optimizing, we need robust correctness tests that will catch regressions.

**Tests to create:**

- [ ] Property-based tests for all FFT sizes (2-8192)
- [ ] Round-trip tests: `ifft(fft(x)) ≈ x`
- [ ] Parseval's theorem: `sum(|x|²) = sum(|X|²)/N`
- [ ] Known-value tests (impulse, sine waves, DC)
- [ ] Linearity: `fft(ax + by) = a*fft(x) + b*fft(y)`
- [ ] Shift theorem: time shift = phase rotation in frequency
- [ ] Comparison against reference implementation (fft.js)

**File:** `tests/fft.correctness.test.js`

### 1.2 Performance Regression Suite

Automated benchmarks that run on every change.

**Metrics to track:**

- ops/sec for each size (64, 256, 1024, 4096)
- Memory bandwidth utilization
- Cache miss rates (if measurable)
- Comparison vs baseline and competitors

**File:** `benchmarks/regression.bench.js`

### 1.3 Profiling Tools

**Tools to create:**

- [ ] Instruction count analyzer (count WASM ops)
- [ ] Memory access pattern visualizer
- [ ] Twiddle factor reuse analyzer
- [ ] Butterfly operation counter

**File:** `tools/profiler.js`

---

## Phase 2: Codelet Generation System

### 2.1 Codelet Generator

Create a tool that generates optimal WAT code for small FFT sizes.

**Approach:**

```
Input: FFT size N (2, 4, 8, 16, 32, 64)
Output: Optimized WAT function for that size
```

**Optimizations to apply:**

1. Eliminate all loops (fully unrolled)
2. Precompute all twiddle factors as constants
3. Minimize temporary variables
4. Reorder operations for instruction pipelining
5. Use FMA where beneficial

**Example output for N=8:**

```wat
(func $fft8 (param $base i32)
  ;; All 8 inputs loaded, all butterflies unrolled
  ;; Twiddles are inline constants
  ;; ~56 adds, ~24 muls for complex 8-point FFT
)
```

**File:** `tools/codelet_generator.js`

### 2.2 Codelet Sizes to Generate

| Size | Radix | Multiplications | Additions | Priority |
| ---- | ----- | --------------- | --------- | -------- |
| 2    | 2     | 0               | 4         | High     |
| 4    | 4     | 0               | 16        | High     |
| 8    | 2×4   | 4               | 52        | High     |
| 16   | 4×4   | 24              | 148       | High     |
| 32   | 2×16  | 88              | 388       | Medium   |
| 64   | 4×16  | 264             | 964       | Medium   |

### 2.3 Codelet Verification

Each generated codelet must pass:

- [ ] Correctness test against reference
- [ ] Performance test (must beat generic loop)
- [ ] Code size check (not too large)

**File:** `tests/codelet.test.js`

---

## Phase 3: Algorithm Improvements

### 3.1 Split-Radix Algorithm

Current: Pure radix-2 (1 butterfly type)
Target: Split-radix (mixed radix-2 and radix-4)

**Benefits:**

- ~33% fewer multiplications than radix-2
- Better instruction-level parallelism

**Implementation steps:**

1. [ ] Implement radix-4 butterfly
2. [ ] Implement split-radix decomposition
3. [ ] Create hybrid that uses radix-4 when N is divisible by 4

**Theoretical improvement:** ~20% faster

### 3.2 Radix-4 Stockham

Pure radix-4 for power-of-4 sizes (4, 16, 64, 256, 1024, 4096).

**Benefits:**

- Twiddles W_N^0 = 1, W_N^(N/4) = -i are trivial
- 25% fewer stages than radix-2
- Better for SIMD (4-way operations)

**File:** `modules/fft_stockham_radix4.wat`

### 3.3 Mixed-Radix Support

For sizes like 12, 24, 48 (products of 2, 3, 4):

- Radix-2 kernel
- Radix-3 kernel
- Radix-4 kernel
- Combine hierarchically

---

## Phase 4: Memory Optimization

### 4.1 Cache-Oblivious Recursion

Instead of iterative stages, use recursive decomposition that naturally fits cache.

```
fft(x, n):
  if n <= CACHE_THRESHOLD:
    use_codelet(x, n)
  else:
    fft(even, n/2)
    fft(odd, n/2)
    combine(even, odd, n)
```

### 4.2 Twiddle Factor Optimization

Current: Precompute all N/2 twiddles
Better:

- Sizes ≤64: inline constants in codelets
- Sizes >64: compute on-the-fly with recurrence

**Twiddle recurrence:**

```
W[k+1] = W[k] * W[1]  (one complex multiply)
```

### 4.3 In-Place vs Out-of-Place

Analyze when in-place (Gentleman-Sande DIF) is better than out-of-place (Stockham).

---

## Phase 5: SIMD Deep Optimization

### 5.1 f32x4 Dual-Complex Operations

Pack 2 complex f32 numbers per v128 register.

**Current:** 1 complex per SIMD op
**Target:** 2 complex per SIMD op (requires algorithm restructuring)

### 5.2 Radix-4 SIMD Butterfly

A radix-4 butterfly naturally processes 4 values, perfect for f32x4.

```wat
;; Process 4 complex values in 2 v128 registers
;; Input: [x0, x1] [x2, x3] as v128 pairs
;; Output: [X0, X1] [X2, X3] with full SIMD utilization
```

### 5.3 Memory Coalescing

Ensure consecutive memory accesses for SIMD loads/stores.

---

## Tooling To Build

### Tool 1: Codelet Generator (`tools/codelet_generator.js`)

```javascript
// Usage: node tools/codelet_generator.js --size 8 --radix 2 --output modules/codelets/
// Generates: fft8.wat with fully unrolled, optimized code

class CodeletGenerator {
  generateFFT(size, options) { ... }
  optimizeForFMA(ast) { ... }
  reorderForPipelining(ast) { ... }
  emitWAT(ast) { ... }
}
```

### Tool 2: Performance Comparator (`tools/perf_compare.js`)

```javascript
// Usage: node tools/perf_compare.js --baseline main --candidate feature-branch
// Output: Performance diff table with statistical significance

async function comparePerformance(baseline, candidate, sizes) {
  // Run both, compute confidence intervals
  // Report: "N=1024: +15% ± 2% (p < 0.01)"
}
```

### Tool 3: Operation Counter (`tools/op_counter.js`)

```javascript
// Usage: node tools/op_counter.js modules/fft_stockham.wat
// Output:
//   f64.mul: 1,234
//   f64.add: 2,345
//   v128.load: 567
//   Total ops per butterfly: 12.3

function countOperations(watFile) {
  // Parse WAT, count by opcode type
}
```

### Tool 4: Memory Access Analyzer (`tools/mem_analyzer.js`)

```javascript
// Instrument WASM to log all memory accesses
// Visualize access patterns, detect cache-unfriendly patterns
```

---

## Test Suite Structure

```
tests/
├── correctness/
│   ├── fft.roundtrip.test.js      # ifft(fft(x)) = x
│   ├── fft.parseval.test.js       # Energy preservation
│   ├── fft.linearity.test.js      # Linearity property
│   ├── fft.shift.test.js          # Shift theorem
│   └── fft.reference.test.js      # Compare to fft.js
├── codelets/
│   ├── codelet.n2.test.js         # 2-point correctness
│   ├── codelet.n4.test.js         # 4-point correctness
│   ├── codelet.n8.test.js         # 8-point correctness
│   └── ...
├── performance/
│   ├── perf.regression.test.js    # Must not regress
│   ├── perf.scaling.test.js       # O(N log N) verification
│   └── perf.memory.test.js        # Memory bandwidth
└── integration/
    ├── real_fft.test.js           # Real FFT specific
    └── streaming.test.js          # Repeated FFT calls
```

---

## Migration Checklist

For each optimization, follow this checklist:

### Pre-Implementation

- [ ] Write correctness tests for affected code paths
- [ ] Establish baseline performance numbers
- [ ] Document expected improvement (with citation if applicable)

### Implementation

- [ ] Implement in isolated module
- [ ] Run correctness tests
- [ ] Run performance benchmarks
- [ ] Compare operation counts

### Post-Implementation

- [ ] All tests pass
- [ ] Performance improved (or explain why not)
- [ ] Code reviewed
- [ ] Documentation updated
- [ ] Benchmark results recorded

---

## Expected Performance Gains (Revised)

### High-Impact Optimizations (from FFTW Analysis)

| Optimization                 | Expected Gain    | Effort | Priority |
| ---------------------------- | ---------------- | ------ | -------- |
| **Twiddle-butterfly fusion** | **+25-40%**      | Medium | **A**    |
| **Fused real-FFT codelets**  | **+20-30% rfft** | Medium | **B**    |
| **DAG-based codelet gen**    | **+10-20%**      | High   | **C**    |
| **Depth-first recursion**    | **+15-25% N≥1K** | Medium | **D**    |
| Register-aware scheduling    | +5-10%           | Medium | E        |

### Original Optimizations

| Optimization      | Expected Gain   | Effort | Priority |
| ----------------- | --------------- | ------ | -------- |
| N=8 codelet       | +15% for N≤64   | Low    | 1        |
| N=16 codelet      | +10% for N≤256  | Low    | 2        |
| Radix-4 Stockham  | +20% overall    | Medium | 3        |
| Split-radix       | +25% overall    | High   | 4        |
| SIMD dual-complex | +30% for f32    | High   | 5        |
| Cache-oblivious   | +10% for N>1024 | Medium | 6        |

### Recommended Implementation Order

To close the 2x gap with fftw-js most efficiently:

1. **Twiddle-butterfly fusion** (Priority A) - Single biggest win, medium effort
2. **Basic codelets** (N=8, N=16) - Low-hanging fruit while building toward fusion
3. **Depth-first recursion** (Priority D) - Large-N improvement
4. **Fused rfft codelets** (Priority B) - Directly targets rfft performance
5. **Radix-4** - Compounds with above optimizations
6. **DAG codelet generator** (Priority C) - Long-term maintainability

**Target:** Match or exceed FFTW-js performance within 10%.

---

## References

1. FFTW Paper: ["The Design and Implementation of FFTW3"](https://www.fftw.org/fftw-paper-ieee.pdf) (Frigo & Johnson, 2005)
2. Genfft Compiler: ["A Fast Fourier Transform Compiler"](https://www.fftw.org/fftw-paper.pdf) (Frigo, PLDI 1999) - Won Most Influential Paper award
3. FFTW Adaptive Architecture: ["FFTW: An Adaptive Software Architecture"](https://www.fftw.org/fftw-paper-icassp.pdf) (Frigo & Johnson, ICASSP)
4. Implementing FFTs: ["Implementing FFTs in Practice"](https://www.csd.uwo.ca/~mmorenom/CS433-CS9624/Resources/Implementing_FFTs_in_Practice.pdf) (S.G. Johnson)
5. SIMD FFT: ["A Portable Short Vector Version of FFTW"](https://users.ece.cmu.edu/~franzf/papers/mathmod.pdf) (Franchetti et al.)
6. Generating Kernels: ["Generating Small FFT Kernels"](<https://eng.libretexts.org/Bookshelves/Electrical_Engineering/Signal_Processing_and_Modeling/Fast_Fourier_Transforms_(Burrus)/10:_Implementing_FFTs_in_Practice/10.06:_Generating_Small_FFT_Kernels>) (Engineering LibreTexts)
7. Split-Radix: "On Computing the Split-Radix FFT" (Sorensen et al., 1986)
8. Stockham: "High-Speed Convolution and Correlation" (Stockham, 1966)
9. fftw-js: [GitHub Repository](https://github.com/j-funk/fftw-js) - FFTW compiled to JS/WASM via Emscripten

---

## Next Steps

1. **Immediate:** Create `tests/correctness/` test suite
2. **Week 1:** Build codelet generator, generate N=8,16 codelets
3. **Week 2:** Implement radix-4 Stockham
4. **Week 3:** Benchmark and iterate
